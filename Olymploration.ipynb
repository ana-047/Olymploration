{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5522cfeb",
   "metadata": {},
   "source": [
    "# Olymploration: Who wins the most medals at the Olympics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc40d4e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Rooted in rich history, symbolism and values associated with the Olympic games, Olympic medals are considered highly prestigious, often representing the pinnacle of athletic achievement. Winning an Olympic medal is a recognition of excellence on the international stage, and it often holds cultural and national significance. It is an opportunity for athletes from all nations, big or small, to transcend national boundaries and compete together. This tradition began in 1896 in Athens, Greece and with the exception of the games not being held due to the two world wars, the Summer games have been held every 4 years. The first Winter Olympics were held in Chamonix, France, in 1924. The goal of this project is to predict which country wins the most medals at the Olympic games. \n",
    "    </div>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c145b",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Data Preprocessing and Exploratory Data Analysis](#EDA)\n",
    "3. [Modeling](#Modeling)\n",
    "    1. [Regression Model](#Regression)\n",
    "    2. [Classification Model](#Classification)\n",
    "4. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e9488",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here is a flowchart of our process:\n",
    "![Alt text](flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe11688",
   "metadata": {},
   "source": [
    "We will now import the packages needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdce843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages needed\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import glob2 as glob\n",
    "import missingno as msno\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, precision_score, recall_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.float_format = '{:,.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e3ad8",
   "metadata": {},
   "source": [
    "We will now import the data files needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in files\n",
    "relative_path = \"data/raw/*.csv\" \n",
    "file_list = glob.glob(relative_path)\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "\n",
    "if not file_list:\n",
    "    print(\"No CSV files found in the specified path.\")\n",
    "else:\n",
    "    file_list.sort() #to stop the order from changing\n",
    "\n",
    "    # Loop through the sorted file list and read each CSV file into a df\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {e}\")\n",
    "\n",
    "#name the dfs\n",
    "df_athletes = dataframes[0]\n",
    "df_event_results = dataframes[1]\n",
    "df_medals = dataframes[2]\n",
    "df_results = dataframes[3]\n",
    "df_country = dataframes[4]\n",
    "df_games = dataframes[5]\n",
    "df_gdp_mean = dataframes[6]\n",
    "df_pop_mean = dataframes[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d9208",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The data for the project was originally sourced from Olympedia which is a website that provides data and statistics on the Olympic games held in the past. The data comprises several csv files which include the following:\n",
    "<br><br>\n",
    "<strong>Games</strong> - venue, start and end dates of historical editions of the Olympic Games\n",
    "<br>\n",
    "<strong>Countries</strong> - list of countries\n",
    "<br>\n",
    "<strong>Athletes</strong> - name, country, height, weight, birthdate, etc.\n",
    "<br>\n",
    "<strong>Medals</strong> - medals tally by country for each year the games were held\n",
    "<br>\n",
    "<strong>Results</strong> - description of results by event\n",
    "<br>\n",
    "<strong>Event Results</strong> - results by event/sport including name of the winner\n",
    "    \n",
    "    \n",
    "<p>As the project unfolded, we actively sought and incorporated supplementary data to enhance the predictive capabilities of our model:</p>\n",
    "<br>\n",
    "    <strong> GDP</strong> - country level GDP \n",
    "    <br>\n",
    "    <strong> Population</strong> - country level population \n",
    "    <br>\n",
    "    <strong> Greenhouse Emissions</strong> - emissions by country  \n",
    "    <br>\n",
    "    <strong> Temperature </strong> - Average temperature by country\n",
    "    <br>\n",
    "    <strong> Incentives </strong> - Incentives awarded to athletes for winning a medal \n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1855384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming the dataframes\n",
    "dataframes = {'df_athletes': df_athletes, \n",
    "       'df_country': df_country, \n",
    "       'df_event_results': df_event_results, \n",
    "       'df_games': df_games, \n",
    "       'df_medals': df_medals, \n",
    "       'df_results': df_results,\n",
    "             'df_gdp_mean' : df_gdp_mean,\n",
    "             'df_pop_mean' : df_pop_mean}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e9218",
   "metadata": {},
   "source": [
    "Here is a glimpse of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "5f9b8d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athlete_id</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>born</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>country</th>\n",
       "      <th>country_noc</th>\n",
       "      <th>description</th>\n",
       "      <th>special_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16809</td>\n",
       "      <td>KÃ¡roly Teppert</td>\n",
       "      <td>Male</td>\n",
       "      <td>1891-07-20</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>HUN</td>\n",
       "      <td>KÃ¡roly Teppert started competing in cycling in the colors of JÃ³barÃ¡t KK in 1911 and moved to MTK in December that year. At the 1912 Stockholm Olympics, he competed in the road race, both individual and team. In addition to sports, Teppert also actively participated in the MTK bicycle division and the Hungarian Cycling Federation. After completing his career, he worked as a post officer.</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43737</td>\n",
       "      <td>Andrzej Socharski</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-08-31</td>\n",
       "      <td>173.0</td>\n",
       "      <td>72</td>\n",
       "      <td>Poland</td>\n",
       "      <td>POL</td>\n",
       "      <td>na</td>\n",
       "      <td>Listed in Olympians Who Won a Medal at the World Shooting Championships (0â1â0 1975 MÃ¼nchen silver: skeet team)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50147</td>\n",
       "      <td>Nathalie Wunderlich</td>\n",
       "      <td>Female</td>\n",
       "      <td>1971-06-03</td>\n",
       "      <td>170.0</td>\n",
       "      <td>50</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>SUI</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5085</td>\n",
       "      <td>Miha Lokar</td>\n",
       "      <td>Male</td>\n",
       "      <td>1935-09-10</td>\n",
       "      <td>182.0</td>\n",
       "      <td>76</td>\n",
       "      <td>Yugoslavia</td>\n",
       "      <td>YUG</td>\n",
       "      <td>na</td>\n",
       "      <td>Listed in Olympians Who Won a Medal at the European Basketball Championship (EuroBasket) (0â1â1 1961 YUG silver; 1963 POL bronze)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136329</td>\n",
       "      <td>Austin Hack</td>\n",
       "      <td>Male</td>\n",
       "      <td>1992-05-17</td>\n",
       "      <td>203.0</td>\n",
       "      <td>100</td>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   athlete_id                 name     sex        born height weight  \\\n",
       "0       16809       KÃ¡roly Teppert    Male  1891-07-20     na     na   \n",
       "1       43737    Andrzej Socharski    Male  1947-08-31  173.0     72   \n",
       "2       50147  Nathalie Wunderlich  Female  1971-06-03  170.0     50   \n",
       "3        5085           Miha Lokar    Male  1935-09-10  182.0     76   \n",
       "4      136329          Austin Hack    Male  1992-05-17  203.0    100   \n",
       "\n",
       "          country country_noc  \\\n",
       "0         Hungary         HUN   \n",
       "1          Poland         POL   \n",
       "2     Switzerland         SUI   \n",
       "3      Yugoslavia         YUG   \n",
       "4   United States         USA   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                             description  \\\n",
       "0  KÃ¡roly Teppert started competing in cycling in the colors of JÃ³barÃ¡t KK in 1911 and moved to MTK in December that year. At the 1912 Stockholm Olympics, he competed in the road race, both individual and team. In addition to sports, Teppert also actively participated in the MTK bicycle division and the Hungarian Cycling Federation. After completing his career, he worked as a post officer.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                     na   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                     na   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                     na   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                     na   \n",
       "\n",
       "                                                                                                                       special_notes  \n",
       "0                                                                                                                                 na  \n",
       "1                    Listed in Olympians Who Won a Medal at the World Shooting Championships (0â1â0 1975 MÃ¼nchen silver: skeet team)  \n",
       "2                                                                                                                                 na  \n",
       "3  Listed in Olympians Who Won a Medal at the European Basketball Championship (EuroBasket) (0â1â1 1961 YUG silver; 1963 POL bronze)  \n",
       "4                                                                                                                                 na  "
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Athletes\n",
    "df_athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "418f9528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_id</th>\n",
       "      <th>edition_url</th>\n",
       "      <th>year</th>\n",
       "      <th>city</th>\n",
       "      <th>country_flag_url</th>\n",
       "      <th>country_noc</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>competition_date</th>\n",
       "      <th>isHeld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1896 Summer Olympics</td>\n",
       "      <td>1</td>\n",
       "      <td>/editions/1</td>\n",
       "      <td>1896</td>\n",
       "      <td>Athina</td>\n",
       "      <td>/images/flags/GRE.png</td>\n",
       "      <td>GRE</td>\n",
       "      <td>6 April</td>\n",
       "      <td>15 April</td>\n",
       "      <td>6 â 13 April</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1900 Summer Olympics</td>\n",
       "      <td>2</td>\n",
       "      <td>/editions/2</td>\n",
       "      <td>1900</td>\n",
       "      <td>Paris</td>\n",
       "      <td>/images/flags/FRA.png</td>\n",
       "      <td>FRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14 May â 28 October</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904 Summer Olympics</td>\n",
       "      <td>3</td>\n",
       "      <td>/editions/3</td>\n",
       "      <td>1904</td>\n",
       "      <td>St. Louis</td>\n",
       "      <td>/images/flags/USA.png</td>\n",
       "      <td>USA</td>\n",
       "      <td>14 May</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 July â 26 November</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1908 Summer Olympics</td>\n",
       "      <td>5</td>\n",
       "      <td>/editions/5</td>\n",
       "      <td>1908</td>\n",
       "      <td>London</td>\n",
       "      <td>/images/flags/GBR.png</td>\n",
       "      <td>GBR</td>\n",
       "      <td>13 July</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27 April â 31 October</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1912 Summer Olympics</td>\n",
       "      <td>6</td>\n",
       "      <td>/editions/6</td>\n",
       "      <td>1912</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>/images/flags/SWE.png</td>\n",
       "      <td>SWE</td>\n",
       "      <td>6 July</td>\n",
       "      <td>27 July</td>\n",
       "      <td>5 May â 27 July</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                edition  edition_id  edition_url  year       city  \\\n",
       "0  1896 Summer Olympics           1  /editions/1  1896     Athina   \n",
       "1  1900 Summer Olympics           2  /editions/2  1900      Paris   \n",
       "2  1904 Summer Olympics           3  /editions/3  1904  St. Louis   \n",
       "3  1908 Summer Olympics           5  /editions/5  1908     London   \n",
       "4  1912 Summer Olympics           6  /editions/6  1912  Stockholm   \n",
       "\n",
       "        country_flag_url country_noc start_date  end_date  \\\n",
       "0  /images/flags/GRE.png         GRE    6 April  15 April   \n",
       "1  /images/flags/FRA.png         FRA        NaN       NaN   \n",
       "2  /images/flags/USA.png         USA     14 May       NaN   \n",
       "3  /images/flags/GBR.png         GBR    13 July       NaN   \n",
       "4  /images/flags/SWE.png         SWE     6 July   27 July   \n",
       "\n",
       "        competition_date isHeld  \n",
       "0           6 â 13 April    NaN  \n",
       "1    14 May â 28 October    NaN  \n",
       "2   1 July â 26 November    NaN  \n",
       "3  27 April â 31 October    NaN  \n",
       "4        5 May â 27 July    NaN  "
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Games\n",
    "df_games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "01961197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_noc</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALG</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASA</td>\n",
       "      <td>American Samoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AND</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_noc         country\n",
       "0         AFG     Afghanistan\n",
       "1         ALB         Albania\n",
       "2         ALG         Algeria\n",
       "3         ASA  American Samoa\n",
       "4         AND         Andorra"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countries\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "44787726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_id</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_noc</th>\n",
       "      <th>gold</th>\n",
       "      <th>silver</th>\n",
       "      <th>bronze</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1896 Summer Olympics</td>\n",
       "      <td>1</td>\n",
       "      <td>1896</td>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896 Summer Olympics</td>\n",
       "      <td>1</td>\n",
       "      <td>1896</td>\n",
       "      <td>Greece</td>\n",
       "      <td>GRE</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1896 Summer Olympics</td>\n",
       "      <td>1</td>\n",
       "      <td>1896</td>\n",
       "      <td>Germany</td>\n",
       "      <td>GER</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1896 Summer Olympics</td>\n",
       "      <td>1</td>\n",
       "      <td>1896</td>\n",
       "      <td>France</td>\n",
       "      <td>FRA</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1896 Summer Olympics</td>\n",
       "      <td>1</td>\n",
       "      <td>1896</td>\n",
       "      <td>Great Britain</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                edition  edition_id  year        country country_noc  gold  \\\n",
       "0  1896 Summer Olympics           1  1896  United States         USA    11   \n",
       "1  1896 Summer Olympics           1  1896         Greece         GRE    10   \n",
       "2  1896 Summer Olympics           1  1896        Germany         GER     6   \n",
       "3  1896 Summer Olympics           1  1896         France         FRA     5   \n",
       "4  1896 Summer Olympics           1  1896  Great Britain         GBR     2   \n",
       "\n",
       "   silver  bronze  total  \n",
       "0       7       2     20  \n",
       "1      18      19     47  \n",
       "2       5       2     13  \n",
       "3       4       2     11  \n",
       "4       3       2      7  "
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#medals\n",
    "df_medals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "75d84f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_id</th>\n",
       "      <th>event_title</th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_id</th>\n",
       "      <th>sport</th>\n",
       "      <th>sport_url</th>\n",
       "      <th>result_location</th>\n",
       "      <th>result_participants</th>\n",
       "      <th>result_format</th>\n",
       "      <th>result_detail</th>\n",
       "      <th>result_description</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9270</td>\n",
       "      <td>7 metres, 1907 Rating, Open</td>\n",
       "      <td>1920 Summer Olympics</td>\n",
       "      <td>7</td>\n",
       "      <td>Sailing</td>\n",
       "      <td>/editions/7/sports/SAL</td>\n",
       "      <td>Oostende</td>\n",
       "      <td>8 from 2 countries</td>\n",
       "      <td>Three races, with final places decided by total points. Point-for-place scoring for each race.</td>\n",
       "      <td>na</td>\n",
       "      <td>There were two yachts competing.  Britainâs Ancora lost the first race to Norwayâs Fornebo, but won the final two races to win the gold medal.\\n</td>\n",
       "      <td>1920-07-07</td>\n",
       "      <td>1920-07-09</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29722</td>\n",
       "      <td>Featherweight (â¤60 kilograms), Men</td>\n",
       "      <td>1980 Summer Olympics</td>\n",
       "      <td>20</td>\n",
       "      <td>Weightlifting</td>\n",
       "      <td>/editions/20/sports/WLF</td>\n",
       "      <td>Dvorets sporta Izmaylovo, Moskva</td>\n",
       "      <td>18 from 14 countries</td>\n",
       "      <td>Total of best lifts in snatch and clean &amp; jerk determined placement.  Ties broken by lightest bodyweight.</td>\n",
       "      <td>na</td>\n",
       "      <td>The favorite was likely Soviet Viktor Mazin who held the world record, although he had come out of nowhere in early 1980. Mazin won the gold medal with the best lift in both snatch and clean &amp; jerk, although his jerk was matched by Bulgariaâs Stefan Dimitrov, who won silver. The bronze medal went to Polandâs Marek Seweryn, who had been the 1979 World Champion and a silver medalist in 1978 as a bantamweight.\\n</td>\n",
       "      <td>1980-07-22</td>\n",
       "      <td>na</td>\n",
       "      <td>14:00-19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54126</td>\n",
       "      <td>Platform, Men</td>\n",
       "      <td>1948 Summer Olympics</td>\n",
       "      <td>12</td>\n",
       "      <td>Diving</td>\n",
       "      <td>/editions/12/sports/DIV</td>\n",
       "      <td>Wembley Arena, Wembley, London</td>\n",
       "      <td>25 from 15 countries</td>\n",
       "      <td>10 metre platform. Final round only. Four compulsory and four optional dives.</td>\n",
       "      <td>na</td>\n",
       "      <td>Sammy Lee won this event, leading in both the preliminary round and the final. Lee had finished third earlier in London on the springboard, which was won by Bruce Harlan, who placed second to Lee in this event. Lee would return in four years at Helsinki and defend his gold medal on platform. The bronze went to Mexican JoaquÃ­n Capilla, who had only been fifth in the preliminaries. Capilla would improve to a silver medal in 1952, behind Lee, and then win the platform gold medal at Melbourne in 1956.\\nLee made it to the top as a diver despite overcoming racial discrimination as a Korean-American. Growing up in Fresno, California, Latinos, Asians and African-Americans were only allowed to use Fresnoâs public Brookside Pool on Wednesdays, on what was called âinternational day,â which was the day before the pool was scheduled to be drained and refilled with clean water. To allow him to practice on other days, his coach dug a pit in his backyard and filled it with sand, and Lee practiced by jumping into the pit.\\n</td>\n",
       "      <td>1948-08-04</td>\n",
       "      <td>1948-08-05</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4684</td>\n",
       "      <td>100 metres Backstroke, Men</td>\n",
       "      <td>1920 Summer Olympics</td>\n",
       "      <td>7</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>/editions/7/sports/SWM</td>\n",
       "      <td>Zwemstadion van Antwerpen, Antwerpen</td>\n",
       "      <td>12 from 6 countries</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>Watten Paoa Kealoha set a world record of 1:14.8 in the second semi-final, breaking the mark of 1:15.6 that had been set by Otto Fahr (GER) in 1912.  Kealoha then won the final easily, defeating three other Americans and Belgiumâs GÃ©rard Blitz.\\n</td>\n",
       "      <td>1920-08-22</td>\n",
       "      <td>1920-08-23</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>Slalom, Men</td>\n",
       "      <td>2002 Winter Olympics</td>\n",
       "      <td>47</td>\n",
       "      <td>Alpine Skiing</td>\n",
       "      <td>/editions/47/sports/ALP</td>\n",
       "      <td>Park City Mountain Resort, Park City, Utah (Know You Don't)</td>\n",
       "      <td>77 from 43 countries</td>\n",
       "      <td>Two runs, total time determined placement.</td>\n",
       "      <td>Jesse Hunt</td>\n",
       "      <td>The last few World Cups had gone to Austrian Benjamin Raich and Norwegian superstar Kjetil AndrÃ© Aamodt, but leading the 2001-2002 World Cup race was Croatian Ivica KosteliÄ, brother of Janica KosteliÄ, who would win three Alpine gold medals at the 2002 Winter Olympics. On the first slalom run, Franceâs Jean-Pierre Vidal, third in the 2001-02 World Cup standings, took the lead over the top American, Bode Miller, with Raich in third and KosteliÄ in fourth. On the second run, Miller caught a gate early and did not finish. The best second run was posted by Vidalâs teammate, SÃ©bastien Amiez, who had been equal eighth after the first run. He was followed by British Scot Alain Baxter, who moved up from a tie with Amiez on the first run. Vidal placed sixth on the second run, good enough to keep him in the lead and win the gold medal, Amiez placing second, and Baxter apparently winning Britainâs first-ever medal in Alpine skiing.\\nBut Baxter had a positive doping test for methamphetamine, the result of using an over-the-counter nasal spray. He was disqualified and Benjamin Raich moved up to the bronze medal position. Baxter appealed his doping positive to the Court of Arbitration for Sport, and was cleared of any intentional attempt at using PEDs, which lifted the potential two-year ban from competition, but he was still disqualified in the Olympic slalom.\\n</td>\n",
       "      <td>2002-02-23</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result_id                         event_title               edition  \\\n",
       "0       9270         7 metres, 1907 Rating, Open  1920 Summer Olympics   \n",
       "1      29722  Featherweight (â¤60 kilograms), Men  1980 Summer Olympics   \n",
       "2      54126                       Platform, Men  1948 Summer Olympics   \n",
       "3       4684          100 metres Backstroke, Men  1920 Summer Olympics   \n",
       "4       1465                         Slalom, Men  2002 Winter Olympics   \n",
       "\n",
       "   edition_id          sport                sport_url  \\\n",
       "0           7        Sailing   /editions/7/sports/SAL   \n",
       "1          20  Weightlifting  /editions/20/sports/WLF   \n",
       "2          12         Diving  /editions/12/sports/DIV   \n",
       "3           7       Swimming   /editions/7/sports/SWM   \n",
       "4          47  Alpine Skiing  /editions/47/sports/ALP   \n",
       "\n",
       "                                               result_location  \\\n",
       "0                                                     Oostende   \n",
       "1                             Dvorets sporta Izmaylovo, Moskva   \n",
       "2                               Wembley Arena, Wembley, London   \n",
       "3                         Zwemstadion van Antwerpen, Antwerpen   \n",
       "4  Park City Mountain Resort, Park City, Utah (Know You Don't)   \n",
       "\n",
       "    result_participants  \\\n",
       "0    8 from 2 countries   \n",
       "1  18 from 14 countries   \n",
       "2  25 from 15 countries   \n",
       "3   12 from 6 countries   \n",
       "4  77 from 43 countries   \n",
       "\n",
       "                                                                                               result_format  \\\n",
       "0             Three races, with final places decided by total points. Point-for-place scoring for each race.   \n",
       "1  Total of best lifts in snatch and clean & jerk determined placement.  Ties broken by lightest bodyweight.   \n",
       "2                              10 metre platform. Final round only. Four compulsory and four optional dives.   \n",
       "3                                                                                                         na   \n",
       "4                                                                 Two runs, total time determined placement.   \n",
       "\n",
       "  result_detail  \\\n",
       "0            na   \n",
       "1            na   \n",
       "2            na   \n",
       "3            na   \n",
       "4    Jesse Hunt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             result_description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              There were two yachts competing.  Britainâs Ancora lost the first race to Norwayâs Fornebo, but won the final two races to win the gold medal.\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The favorite was likely Soviet Viktor Mazin who held the world record, although he had come out of nowhere in early 1980. Mazin won the gold medal with the best lift in both snatch and clean & jerk, although his jerk was matched by Bulgariaâs Stefan Dimitrov, who won silver. The bronze medal went to Polandâs Marek Seweryn, who had been the 1979 World Champion and a silver medalist in 1978 as a bantamweight.\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                Sammy Lee won this event, leading in both the preliminary round and the final. Lee had finished third earlier in London on the springboard, which was won by Bruce Harlan, who placed second to Lee in this event. Lee would return in four years at Helsinki and defend his gold medal on platform. The bronze went to Mexican JoaquÃ­n Capilla, who had only been fifth in the preliminaries. Capilla would improve to a silver medal in 1952, behind Lee, and then win the platform gold medal at Melbourne in 1956.\\nLee made it to the top as a diver despite overcoming racial discrimination as a Korean-American. Growing up in Fresno, California, Latinos, Asians and African-Americans were only allowed to use Fresnoâs public Brookside Pool on Wednesdays, on what was called âinternational day,â which was the day before the pool was scheduled to be drained and refilled with clean water. To allow him to practice on other days, his coach dug a pit in his backyard and filled it with sand, and Lee practiced by jumping into the pit.\\n   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Watten Paoa Kealoha set a world record of 1:14.8 in the second semi-final, breaking the mark of 1:15.6 that had been set by Otto Fahr (GER) in 1912.  Kealoha then won the final easily, defeating three other Americans and Belgiumâs GÃ©rard Blitz.\\n   \n",
       "4  The last few World Cups had gone to Austrian Benjamin Raich and Norwegian superstar Kjetil AndrÃ© Aamodt, but leading the 2001-2002 World Cup race was Croatian Ivica KosteliÄ, brother of Janica KosteliÄ, who would win three Alpine gold medals at the 2002 Winter Olympics. On the first slalom run, Franceâs Jean-Pierre Vidal, third in the 2001-02 World Cup standings, took the lead over the top American, Bode Miller, with Raich in third and KosteliÄ in fourth. On the second run, Miller caught a gate early and did not finish. The best second run was posted by Vidalâs teammate, SÃ©bastien Amiez, who had been equal eighth after the first run. He was followed by British Scot Alain Baxter, who moved up from a tie with Amiez on the first run. Vidal placed sixth on the second run, good enough to keep him in the lead and win the gold medal, Amiez placing second, and Baxter apparently winning Britainâs first-ever medal in Alpine skiing.\\nBut Baxter had a positive doping test for methamphetamine, the result of using an over-the-counter nasal spray. He was disqualified and Benjamin Raich moved up to the bronze medal position. Baxter appealed his doping positive to the Court of Arbitration for Sport, and was cleared of any intentional attempt at using PEDs, which lifted the potential two-year ban from competition, but he was still disqualified in the Olympic slalom.\\n   \n",
       "\n",
       "   start_date    end_date         time  \n",
       "0  1920-07-07  1920-07-09           na  \n",
       "1  1980-07-22          na  14:00-19:00  \n",
       "2  1948-08-04  1948-08-05           na  \n",
       "3  1920-08-22  1920-08-23           na  \n",
       "4  2002-02-23          na           na  "
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "af128205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_id</th>\n",
       "      <th>country_noc</th>\n",
       "      <th>sport</th>\n",
       "      <th>event</th>\n",
       "      <th>result_id</th>\n",
       "      <th>athlete</th>\n",
       "      <th>athlete_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>medal</th>\n",
       "      <th>isTeamSport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908 Summer Olympics</td>\n",
       "      <td>5</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>Athletics</td>\n",
       "      <td>100 metres, Men</td>\n",
       "      <td>56265</td>\n",
       "      <td>Ernest Hutcheon</td>\n",
       "      <td>64710</td>\n",
       "      <td>DNS</td>\n",
       "      <td>na</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1908 Summer Olympics</td>\n",
       "      <td>5</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>Athletics</td>\n",
       "      <td>400 metres, Men</td>\n",
       "      <td>56313</td>\n",
       "      <td>Henry Murray</td>\n",
       "      <td>64756</td>\n",
       "      <td>DNS</td>\n",
       "      <td>na</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1908 Summer Olympics</td>\n",
       "      <td>5</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>Athletics</td>\n",
       "      <td>800 metres, Men</td>\n",
       "      <td>56338</td>\n",
       "      <td>Harvey Sutton</td>\n",
       "      <td>64808</td>\n",
       "      <td>3 h8 r1/2</td>\n",
       "      <td>na</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1908 Summer Olympics</td>\n",
       "      <td>5</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>Athletics</td>\n",
       "      <td>800 metres, Men</td>\n",
       "      <td>56338</td>\n",
       "      <td>Guy Haskins</td>\n",
       "      <td>922519</td>\n",
       "      <td>DNS</td>\n",
       "      <td>na</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1908 Summer Olympics</td>\n",
       "      <td>5</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>Athletics</td>\n",
       "      <td>800 metres, Men</td>\n",
       "      <td>56338</td>\n",
       "      <td>Joseph Lynch</td>\n",
       "      <td>64735</td>\n",
       "      <td>DNS</td>\n",
       "      <td>na</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                edition  edition_id country_noc      sport            event  \\\n",
       "0  1908 Summer Olympics           5         ANZ  Athletics  100 metres, Men   \n",
       "1  1908 Summer Olympics           5         ANZ  Athletics  400 metres, Men   \n",
       "2  1908 Summer Olympics           5         ANZ  Athletics  800 metres, Men   \n",
       "3  1908 Summer Olympics           5         ANZ  Athletics  800 metres, Men   \n",
       "4  1908 Summer Olympics           5         ANZ  Athletics  800 metres, Men   \n",
       "\n",
       "   result_id          athlete  athlete_id        pos medal  isTeamSport  \n",
       "0      56265  Ernest Hutcheon       64710        DNS    na        False  \n",
       "1      56313     Henry Murray       64756        DNS    na        False  \n",
       "2      56338    Harvey Sutton       64808  3 h8 r1/2    na        False  \n",
       "3      56338      Guy Haskins      922519        DNS    na        False  \n",
       "4      56338     Joseph Lynch       64735        DNS    na        False  "
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#event results\n",
    "df_event_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a78e25",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8929be2",
   "metadata": {},
   "source": [
    "Before we begin analyzing, we would like to check each dataframe for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22639711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values\n",
    "def check_missing_values(df):\n",
    "    return df.isnull().sum()\n",
    "\n",
    "\n",
    "# check_missing_values(df_event_results)\n",
    "# check_missing_values(df_country)\n",
    "# check_missing_values(df_games)\n",
    "# check_missing_values(df_athletes)\n",
    "# check_missing_values(df_medals)\n",
    "# check_missing_values(df_results)\n",
    "\n",
    "def count_na(df):\n",
    "    def helper(val):\n",
    "        return val == 'na'\n",
    "    return df.applymap(helper).sum() / df.shape[0]\n",
    "\n",
    "# for key, value in dataframes.items():\n",
    "#     print(f'\\n\\n{key}\\n\\n')\n",
    "#     print(count_na(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eed8f6",
   "metadata": {},
   "source": [
    "There are missing values for end_Date, start_date if the games were not held. These missing values will not affect our analysis.\n",
    "<br> <br>\n",
    "Let's check the data to see if it is equally distributed between the Summer and Winter games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have winter olympics available as much? \n",
    "dict(Counter([i[5:] for i in df_games['edition'].unique()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4851fa",
   "metadata": {},
   "source": [
    "Can an athlete take part in several events?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one athlete can take part in several events\n",
    "df_event_results.groupby(['edition', 'country_noc'])['athlete'].unique().reset_index(name='unique_athletes').assign(athlete_count=lambda x: x['unique_athletes'].apply(len)).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcd1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total medals \n",
    "\n",
    "df_medals_agg = df_medals.groupby('country')[['gold', 'silver', 'bronze', 'total']].sum()\n",
    "df_medals_agg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16567f5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "<strong><i>Who has won the most medals? </i> </strong>\n",
    "\n",
    "Historically, USA, the Soviet Union, Germany, Great Britain and France have won the most medals at the Olympics.The Soviet Union won the second most at 1204 followed by Germany at 1098. In terms of gold and silver medals won, USA leads by a mile.  USA should be considered an outlier while modeling.  The table above shows the descriptive statistics for the medals data. As you can see, the lower 75% of countries have historically won a total of only 98 medals which confirms that only a handful of countries dominate at the games.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#historically, who won the most medals?? \n",
    "\n",
    "df_medals2 = df_medals.groupby('country_noc')[['total','gold', 'silver', 'bronze']].sum()\n",
    "df_medals2.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "top5_rows = df_medals2.nlargest(5, 'total')\n",
    "\n",
    "top5_countries = top5_rows['country_noc'].tolist()\n",
    "top5_medals = top5_rows['total'].tolist()\n",
    "\n",
    "print(f\"The top 5 countries are: {top5_countries}\")\n",
    "print(f\"The corresponding counts are: {top5_medals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to long format \n",
    "top_50 = df_medals2.nlargest(50, 'total')\n",
    "df_medals3 = top_50.drop(['total'], axis=1)\n",
    "df_medals_melted = pd.melt(df_medals3, id_vars='country_noc', var_name='Medal', value_name='Count')\n",
    "\n",
    "df_medals_melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbeb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot top 50 countries that won medals\n",
    "df_stacked = df_medals_melted.pivot_table(index='country_noc', columns='Medal', values='Count', fill_value=0)\n",
    "\n",
    "df_stacked_sorted = df_stacked.sum(axis=1).sort_values(ascending=False).index\n",
    "df_stacked = df_stacked.loc[df_stacked_sorted]\n",
    "\n",
    "\n",
    "colors = {'gold': 'gold', 'silver': 'silver', 'bronze': 'peru'}\n",
    "\n",
    "\n",
    "bar_width = 0.8\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "bottom_values = np.zeros(len(df_stacked.index))\n",
    "\n",
    "for medal_type, color in colors.items():\n",
    "    plt.bar(df_stacked.index, df_stacked[medal_type], color=color, label=medal_type, width=bar_width, bottom=bottom_values)\n",
    "    bottom_values += df_stacked[medal_type]  \n",
    "\n",
    "plt.title('Top 50 Countries by Medal Type: USA dominates')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Medals')\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.legend(title='Medal Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076bbfc",
   "metadata": {},
   "source": [
    "<strong><i> Has anything changed in the past 20 years? Do countries continue to remain dominant?</i></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#has anything changed in the past 20 years?\n",
    "\n",
    "#filter df to include 2004 Athens games onwards\n",
    "\n",
    "df_last20 = df_medals[['year', 'country', 'total']].copy()\n",
    "\n",
    "df_last20 = df_last20.loc[df_last20['year'] >= 1992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_by_country = df_last20.groupby('country')['total'].sum()\n",
    "top_10_countries = total_by_country.sort_values(ascending=False).head(12)\n",
    "top_10_countries_list = total_by_country.sort_values(ascending=False).head(12).index.tolist()\n",
    "\n",
    "recent_games  = df_last20[df_last20['country'].isin(top_10_countries_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x='year', y='total', hue='country', data=recent_games, marker='o', markersize=8)\n",
    "\n",
    "plt.xticks(recent_games['year'].unique())\n",
    "\n",
    "plt.title('Total Medals by Country in the past 20 years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Medals')\n",
    "plt.legend(title='Country', bbox_to_anchor=(1.05, 1), loc='upper left')  \n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7987f87",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "In the past 30 years, USA continues to remain extremely dominant at the Summer Olympics (first Summer Olympics in the graph took place in 1992 and first Winter Olympics took place in 1994). The Russian Federation/Soviet Union which historically had the second most medals is not as dominant as before. In the past 4 editions of the Summer games, China has won the second most number of medals, bypassing Russia. Great Britain has also steadily increased their medal count in the past 30 years.\n",
    "However at the Winter Olympics, Norway's athletes have been outpacing the U.S. and Germany.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c361f60d",
   "metadata": {},
   "source": [
    "<strong> <i>Do GDP and Population have an impact on the number of medals won?<i><strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e8bb9",
   "metadata": {},
   "source": [
    "We are going to use the past 10 year avg as a proxy to check the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ed99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDP and population \n",
    "df_country_gdp = pd.read_csv('data/raw/country_gdp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_gdp.drop(['Indicator Name', 'Indicator Code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85118b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country2 = pd.merge(df_country, df_country_gdp, how='left', left_on='country', right_on='Country Name')\n",
    "\n",
    "exclude_columns = ['country_noc', 'country', 'Country Code', 'Country Name']\n",
    "numeric_columns = df_country2.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Filter out columns to exclude from the division\n",
    "columns_to_divide = numeric_columns.difference(exclude_columns)\n",
    "\n",
    "# Perform division only on numeric columns\n",
    "df_country2[columns_to_divide] = df_country2[columns_to_divide].div(1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country2.iloc[:, -10:] = df_country2.iloc[:, -10:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# # Calculate the mean of the last 10 columns while ignoring NaN values\n",
    "df_country2['mean_gdp_last_10yr_bn'] = df_country2.iloc[:, -10:].mean(axis=1, skipna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_gdp = df_country2['mean_gdp_last_10yr_bn'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f69032",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "We would like to use GDP and Population  as additional independent variables in our models and examine whether it has an impact on the medals tally or not. One of the limitations we are facing with the World Bank data is that it is available only 1960 onwards with data for some countries being available much later. The data for the past 10 years is available and we plan on using the 10yr mean as a proxy for GDP in our model. An alternative way is to create dummy variables after categorizing the countries based on the range of their population, eg- high, medium, low. The chart below shows the countries with the highest mean GDP over the past 10 years and  we guess that GDP  is positively correlated with the medals count.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df_country2.sort_values(by='mean_gdp_last_10yr_bn', ascending=False)\n",
    "top_10_values = df_sorted.head(10)\n",
    "\n",
    "plt.barh(top_10_values['country_noc'], top_10_values['mean_gdp_last_10yr_bn'], color='teal')\n",
    "plt.xlabel('GDP($bn)')\n",
    "plt.title('Mean GDP over the last 10 years by country (in $bn), USA leads...')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd53304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = pd.read_csv('data/raw/country_population.csv')\n",
    "df_pop.drop(['Country Name',  'Indicator Name', 'Indicator Code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pop.drop(['Indicator Name', 'Indicator Code'], axis=1, inplace=True)\n",
    "df_pop2 = pd.merge(df_country, df_pop, how='left', left_on='country', right_on='Country_Updated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581acc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['country_noc', 'country', 'Country Code', 'Country_Updated']\n",
    "df_pop2.loc[:, df_pop2.columns.difference(exclude_columns)] /= 1e6 # convert to millions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#following a similar approach of taking the 10 year mean\n",
    "df_pop2['mean_pop_last_10yr_mm'] = df_pop2.iloc[:, -10:].mean(axis=1)\n",
    "df_country_pop = df_pop2[['country_noc', 'country', 'Country Code', 'mean_pop_last_10yr_mm']].copy()\n",
    "#df_country_pop.to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8548a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_pop = df_country_pop['mean_pop_last_10yr_mm'].isnull().sum()\n",
    "missing_values_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddee18c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "We followed a similar approach for Population and calculated the 10yr mean as a proxy. Apart from China, USA, Russia, Japan, other countries with the highest mean population generally do not do well at the Games. Having a larger population could also mean less resources to invest in training athletes and could in fact, have a negative impact on medals won.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35556588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_pop = df_country_pop.sort_values(by='mean_pop_last_10yr_mm', ascending=False)\n",
    "top_10_values_pop = df_sorted_pop.head(10)\n",
    "\n",
    "\n",
    "plt.barh(top_10_values_pop['country_noc'], top_10_values_pop['mean_pop_last_10yr_mm'], color='salmon')\n",
    "plt.xlabel('Population($MM)')\n",
    "plt.title('Mean Population over the last 10 years by country')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by total medals for each country\n",
    "\n",
    "df_grouped = df_medals.groupby('country_noc')['total'].sum()\n",
    "df_grouped = pd.DataFrame(df_grouped)\n",
    "df_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5749f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check correlation between variables and medals tally\n",
    "\n",
    "df_merged = pd.merge(df_grouped, df_country2[['country_noc','mean_gdp_last_10yr_bn']\n",
    "                                                                        ],how='left', on='country_noc')\n",
    "\n",
    "df_merged2 = pd.merge(df_merged, df_country_pop[['country_noc','mean_pop_last_10yr_mm']\n",
    "                                                                        ],how='left', on='country_noc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude China, India, USA\n",
    "excluded_countries = ['CHN', 'IND', 'USA']\n",
    "df_filtered = df_merged2[~df_merged2['country_noc'].isin(excluded_countries)]\n",
    "sns.scatterplot(y='total', x='mean_pop_last_10yr_mm', data=df_filtered, color='salmon')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Population (in MM)')\n",
    "plt.ylabel('Medals Won')\n",
    "plt.title('Population vs Medals won')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e72131",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "We excluded China and USA along with India which has the second highest 10 yr mean population and it appears that some countries with smaller populations have won more medals. However, there are also countries that are very tiny, have very small populations and hence, have very low participation at the Olympics.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad29976",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y='total', x='mean_gdp_last_10yr_bn', data=df_filtered, color='teal')\n",
    "# Add labels and title\n",
    "plt.xlabel('GDP (in $bn)')\n",
    "plt.ylabel('Medals Won')\n",
    "plt.title('GDP vs Medals won')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b063744",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The scatter plot confirms that GDP is positvely correlated with medals one. We still have countries with missing GDP but they are either very small or nations that had a name change, we will be excluding those records.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49dabd",
   "metadata": {},
   "source": [
    "<strong>Let us get the sport for each athlete...</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join athletes with events to get the sport for each athlete, it is not in df_athletes\n",
    "\n",
    "df_athletes_sport = pd.merge(df_athletes[['name', 'sex', 'height', 'weight']],\n",
    "                              df_event_results[['athlete', 'sport']], left_on='name', right_on='athlete', how='left')\n",
    "\n",
    "df_athletes_sport.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athletes_sport = df_athletes_sport.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11781828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athletes_sport['height'] = pd.to_numeric(df_athletes_sport['height'], errors='coerce')  \n",
    "df_athletes_sport['weight'] = pd.to_numeric(df_athletes_sport['weight'], errors='coerce')  \n",
    "\n",
    "# Group by 'sport' and calculate the average height and weight\n",
    "average_data = df_athletes_sport.groupby(['sport', 'sex'])[['height', 'weight']].mean().reset_index()\n",
    "\n",
    "#select a few sports \n",
    "\n",
    "select_sports = ['Archery', 'Artistic Gymnastics', 'Athletics',\n",
    "                'Badminton', 'Boxing', 'Cross Country Skiing',\n",
    "                'Cycling Track', 'Diving', 'Equestrian Dressage',\n",
    "                'Fencing', 'Hockey', 'Judo',\n",
    "                 'Rowing', 'Shooting', 'Handball',\n",
    "                 'Swimming', 'Triathlon', 'Volleyball',\n",
    "                 'Weightlifting', 'Wrestling', 'Basketball', 'Table Tennis', 'Tennis'\n",
    "                ]\n",
    "\n",
    "#keep only selected sports\n",
    "mask = average_data['sport'].isin(select_sports) \n",
    "selected_df = average_data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b472f",
   "metadata": {},
   "source": [
    "<strong><i>How does an athleteâs height and weight affect their choice of sport?<strong><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f62983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort  in descending order\n",
    "average_height_by_sport = selected_df.sort_values(by='height', ascending=False)\n",
    "average_height_by_sport.dropna(inplace=True)\n",
    "\n",
    "colors = {'Female': 'thistle', 'Male': 'skyblue'}\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.barplot(y='sport', x='height', hue='sex', data=average_height_by_sport, palette=colors)\n",
    "plt.title('AvHeight by Sport: Basketball players are the tallest')\n",
    "plt.xlabel('Average Height in cm')\n",
    "plt.ylabel('')\n",
    "plt.legend(title='Gender', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort weight in descending order\n",
    "average_weight_by_sport = selected_df.sort_values(by='weight', ascending=False)\n",
    "average_weight_by_sport.dropna(inplace=True)\n",
    "\n",
    "colors = {'Female': 'thistle', 'Male': 'skyblue'}\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.barplot(y='sport', x='weight', hue='sex', data=average_weight_by_sport, palette=colors)\n",
    "plt.title('Average Weight by Sport: Basketball players also weigh the most')\n",
    "plt.xlabel('Average Weight in kg')\n",
    "plt.ylabel('')\n",
    "plt.legend(title='Gender', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20716ae",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "We selected some of the most popular events at the Olympics to see if there is any obvious trend in the height and weight of athletes competing in those sports. The charts above show the average height and weight of all the athletes playing a particular sport, missing records were excluded. Basketball players tend to be the tallest and also weigh the most but the difference in the average height across sports is minimal for both males and females. On the other hand, the difference in weight between males and females across the different sports is more obvious. Furthermore, gymnasts, boxers and divers weigh less than athletes playing basketball or handball.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578a2f9",
   "metadata": {},
   "source": [
    "<strong><i>Which countries do better at team vs individual events?</i></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643492d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#which countries do better at team vs individual events\n",
    "\n",
    "df_event_results2 = df_event_results[df_event_results['medal'] != 'na']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d621e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by country\n",
    "\n",
    "df_team = df_event_results2.groupby(['country_noc', 'isTeamSport']).result_id.count().reset_index()\n",
    "df_team = pd.DataFrame(df_team)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by team vs individual\n",
    "filtered_data = df_team[df_team['isTeamSport'] == False]\n",
    "\n",
    "top_20_countries = filtered_data.groupby('country_noc')['result_id'].sum().nlargest(20).index\n",
    "\n",
    "top_20_data = filtered_data[filtered_data['country_noc'].isin(top_20_countries)]\n",
    "\n",
    "top_20_data = top_20_data.sort_values(by='result_id', ascending=False)\n",
    "\n",
    "#lollipop chart\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.stem(top_20_data['country_noc'], top_20_data['result_id'], basefmt='k-', use_line_collection=True, markerfmt='purple',\n",
    "         linefmt='purple')\n",
    "plt.title('Top 20 Countries winning Individual Events')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Events won')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2471e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data2 = df_team[df_team['isTeamSport'] == True]\n",
    "\n",
    "top_20_countries2 = filtered_data2.groupby('country_noc')['result_id'].sum().nlargest(20).index\n",
    "\n",
    "\n",
    "top_20_data2 = filtered_data2[filtered_data2['country_noc'].isin(top_20_countries2)]\n",
    "\n",
    "# Sort the data for better visualization\n",
    "top_20_data2 = top_20_data2.sort_values(by='result_id', ascending=False)\n",
    "\n",
    "# Create a lollipop chart\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.stem(top_20_data2['country_noc'], top_20_data2['result_id'], basefmt='k-', use_line_collection=True, markerfmt='purple',\n",
    "         linefmt='purple')\n",
    "plt.title('Top 20 Countries winning Team Events')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Events won')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47f855",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "While the USA is an outlier doing well at both individual and team events, Great Britain comes next in terms of most medals won at team events. Countries like Canada, Australia, the Netherlands among others also happen to do better at team events than individual events.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74a8af",
   "metadata": {},
   "source": [
    "<strong><i>Who is the greatest Olympian of all time?<strong><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe64837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#greatest Olympian of all time \n",
    "\n",
    "greatest_olympian = df_event_results2.groupby(['athlete', 'sport', 'country_noc']).result_id.count().sort_values(ascending=False)\n",
    "greatest_olympian = pd.DataFrame(greatest_olympian)\n",
    "greatest_olympian.rename({'result_id':'medals_won'}, axis=1, inplace=True)\n",
    "greatest_olympian.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "greatest_olympian2 = greatest_olympian.sort_values(by='medals_won', ascending=False)\n",
    "greatest_olympian2 = greatest_olympian2.head(20)\n",
    "plt.figure(figsize=(6, 5))\n",
    "bars = plt.barh(greatest_olympian2['athlete'], greatest_olympian2['medals_won'], color='darkslateblue')\n",
    "plt.xlabel('Medals Won')\n",
    "plt.ylabel('Athlete')\n",
    "plt.title('Greatest Olympians of All Time by Medals Won')\n",
    "\n",
    "for bar, label in zip(bars, greatest_olympian2['country_noc']):\n",
    "    plt.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height() / 2, label, ha='left', va='center', color='black', fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a9bc4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Michael Phelps is well-known as the greatest Olympian of all time but in second place is Larisa Latynina, a gymnast from the former Soviet Union who won 18 medals. 25% of the top 20 are from the USA.\n",
    "    <div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3825111",
   "metadata": {},
   "source": [
    "<strong><i>Which gender wins more medals? </strong></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#which gender wins more medals\n",
    "gender_medals = pd.merge(df_athletes[['name', 'sex']], df_event_results2[['athlete','medal', 'sport','result_id']],\n",
    "                        left_on=\"name\", right_on=\"athlete\")\n",
    "gender_medals2 = gender_medals.groupby('sex').athlete.count()\n",
    "gender_medals2=pd.DataFrame(gender_medals2)\n",
    "gender_medals2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.pie(gender_medals2['athlete'], labels=gender_medals2.sex, autopct='%1.1f%%', startangle=90,\n",
    "        colors=['thistle', 'skyblue'])\n",
    "\n",
    "\n",
    "plt.title('Medal Winners by Gender')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gold medal winners\n",
    "\n",
    "gender_gold = gender_medals[gender_medals['medal'] == 'Gold']\n",
    "\n",
    "gender_gold2 = gender_gold.groupby('sex').athlete.count()\n",
    "gender_gold2=pd.DataFrame(gender_gold2)\n",
    "gender_gold2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fd9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.pie(gender_gold2['athlete'], labels=gender_gold2.sex, autopct='%1.1f%%', startangle=90,\n",
    "        colors=['thistle', 'skyblue'])\n",
    "\n",
    "\n",
    "plt.title('Gold Medal Winners by Gender')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42898c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "72% of all medal winners are Male, with a similar stat for gold medal winners. This is expected given that the historical participation rate of female athletes  is 26%.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babaea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_all = df_athletes.groupby('sex')['name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many athletes take part by country\n",
    "df_event_results.groupby(['edition', 'country_noc'], as_index=False)['athlete'].count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one athlete can take part in several events\n",
    "df_event_results.groupby(['edition', 'country_noc'])['athlete'].unique().reset_index(name='unique_athletes').assign(athlete_count=lambda x: x['unique_athletes'].apply(len)).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7ea99",
   "metadata": {},
   "source": [
    "<strong>Merge: Join medals and games</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_medals['edition_id'].nunique()\n",
    "# df_games['edition_id'].nunique()\n",
    "\n",
    "\n",
    "# drop war and future events\n",
    "df_games = df_games.drop(df_games[~df_games['edition_id'].isin(df_medals['edition_id'])].index)\n",
    "\n",
    "\n",
    "df_games_medals = pd.merge(df_medals, pd.merge(df_games, df_country, on='country_noc', how='left'), on=['edition_id', 'edition', 'year'], how='left',\n",
    "        suffixes=('_performing', '_host'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#special or one time events\n",
    "df_games_medals[(df_games_medals['edition_id'] == 4)|(df_games_medals['edition_id'] == 48)]\n",
    "\n",
    "# drop the one-time events\n",
    "df_games_medals = df_games_medals.drop(df_games_medals[(df_games_medals['edition_id'] == 4)|\n",
    "                                                       (df_games_medals['edition_id'] == 48)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games_medals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb966839",
   "metadata": {},
   "source": [
    "Let us do some additional EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some more eda\n",
    "\n",
    "def eda_visualization(df):\n",
    "    numerical_columns = df.select_dtypes(include='number').columns\n",
    "    categorical_columns = df.select_dtypes(exclude='number').columns\n",
    "\n",
    "    for column in numerical_columns:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[column], kde=True)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        ax = sns.boxplot(data=df, x=column, showfliers=False)\n",
    "        plt.title(f'Boxplot of {column} Without Outliers')\n",
    "        \n",
    "        def add_quartile_labels(ax):\n",
    "            for line in ax.lines:\n",
    "                x, y = line.get_xydata()[0]  \n",
    "                ax.text(x, y, f\"{x:.0f}\", ha='left', va='center')\n",
    "        \n",
    "        add_quartile_labels(ax)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        categories_to_show = df[column].value_counts().index[:30]\n",
    "\n",
    "        if not categories_to_show.empty:\n",
    "            sns.countplot(data=df[df[column].isin(categories_to_show)], x=column, order=categories_to_show)\n",
    "\n",
    "            for idx, count in enumerate(df[df[column].isin(categories_to_show)][column].value_counts()):\n",
    "                plt.text(idx, count + 1, str(count), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "            plt.title(f'Count of each category in {column}')\n",
    "            plt.xticks(rotation=90, ha='right', fontsize=7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No categories to show for column {column}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9b208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eda_visualization(df_games_medals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ecd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#further analyze df_games_medals \n",
    "df_comparison = df_games_medals.groupby(['country_host', 'edition', 'year', 'country_performing'], as_index=False)[['total', 'gold', 'silver', 'bronze']].sum() \\\n",
    "    .groupby('edition', group_keys=False) \\\n",
    "    .apply(lambda x: x.nlargest(8,'total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dummy variabe for season\n",
    "df_comparison['season'] = [1 if 'Winter' in edition else 0 for edition in df_comparison['edition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76349634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y='total', x='mean_pop_last_10yr_mm', data=df_filtered, color='salmon')#add another dummy variable for is_host\n",
    "\n",
    "df_comparison['is_host'] = np.where(df_comparison['country_host'] == df_comparison['country_performing'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b154255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summer = df_comparison[df_comparison['season'] == 0]\n",
    "df_summer2 = df_summer.groupby(['year', 'is_host'])['total'].sum()\n",
    "df_summer2 = pd.DataFrame(df_summer2)\n",
    "df_summer2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdcae5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "<strong><i>Does being a host country appear to furnish an advantage?</i></strong>\n",
    "<br>\n",
    "For most years, the host country wins medals. The number of medals won by the host country appears to be more sizable during the summer games than the winter ones. Though some countries are repeatedly hosting the games and could simply be strong contenders, it is likely still an influential factor. We will include is_host and season as two binary variables to capture this potential effect. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot summer games\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='total', y='year', hue='is_host', data=df_summer2, dodge=False, ci=None,palette={1: 'mediumvioletred', 0: 'lightseagreen'}, orient='h')\n",
    "plt.title('Summer Games: Total Medal Count by Year')\n",
    "plt.xlabel('Total Medals')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85208a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot winter games\n",
    "\n",
    "df_winter = df_comparison[df_comparison['season'] == 1]\n",
    "df_winter2 = df_winter.groupby(['year', 'is_host'])['total'].sum()\n",
    "df_winter2 = pd.DataFrame(df_winter2)\n",
    "df_winter2.reset_index(inplace=True)\n",
    "df_winter2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='total', y='year', hue='is_host', data=df_winter2, dodge=False, ci=None,palette={1: 'cornflowerblue', 0: 'peachpuff'}, orient='h')\n",
    "plt.title('Winter Games:Total Medal Count by Year')\n",
    "plt.xlabel('Total Medals')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcfb75f",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e99d0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "<strong> Preliminary baseline model:</strong>\n",
    "\n",
    "We will now build a baseline model to predict how many medals a country will win at the Olympics.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dcc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function toplot residuals \n",
    "def plot_residuals(actual_values, predicted_values, title='Residual Plot'):  \n",
    "    sns.set_style('darkgrid')\n",
    "    residuals = actual_values - predicted_values\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.scatterplot(x=actual_values, y=residuals, color='lightblue')\n",
    "    plt.axhline(y=0, color='red', linestyle='--')\n",
    "\n",
    "    plt.title(f'{title}', fontsize=12)\n",
    "    plt.xlabel('Actual Values', fontsize=10)\n",
    "    plt.ylabel('Residuals', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_results(model):\n",
    "    global results\n",
    "    results.append({\n",
    "        'model':f'{model}',\n",
    "        'MSE train': f'{mse_train}',\n",
    "        'MSE test': f'{mse_test}',\n",
    "        'RMSE train': f'{np.sqrt(mse_train)}',\n",
    "        'RMSE test': f'{np.sqrt(mse_test)}',\n",
    "        'R2 train': f'{r2_score(y_train, base_train_predictions)}',\n",
    "        'R2 test': f'{r2_score(y_test, base_predictions)}'\n",
    "    })\n",
    "    print('results recorded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fad927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a baseline model\n",
    "\n",
    "#Model 1: Using data for the 2020 Olympics only\n",
    "\n",
    "#start building df to predict medal count at the 2024 summer games\n",
    "\n",
    "df_games_medals['is_host'] = np.where(df_games_medals['country_host'] == df_games_medals['country_performing'], 1, 0)\n",
    "\n",
    "#add gdp and population-using already processed df saved in the data folder\n",
    "\n",
    "df_gdp_pop = pd.merge(df_gdp_mean,df_pop_mean, on='country_noc', how='left')\n",
    "df_gdp_pop.drop(['country_x', 'Country Code_x', 'country_y', 'Country Code_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ac2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols for model\n",
    "df_cols = df_games_medals[['year','edition', 'country_noc_performing', 'gold', 'silver', 'bronze','total', 'is_host']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols.rename({'country_noc_performing':'country_noc'}, axis=1, inplace=True)\n",
    "df_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "df_model = pd.merge(df_cols, df_gdp_pop[['country_noc','mean_gdp_last_10yr_bn', 'mean_pop_last_10yr_mm']], on='country_noc', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebced0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add contingent size \n",
    "contingent_size = df_event_results.groupby(['edition', 'country_noc'], as_index=False)['athlete'].nunique()\n",
    "\n",
    "df_model2 = pd.merge(df_model, contingent_size[['edition','country_noc','athlete']], on=['edition','country_noc'], how='left')\n",
    "df_model2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model2.rename({'athlete':'contingent_size'}, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of events they participate in\n",
    "\n",
    "event_participation =  df_event_results.groupby(['edition', 'country_noc'], \n",
    "                                                                 as_index=False)['event'].nunique()\n",
    "\n",
    "df_model3 = pd.merge(df_model2, event_participation[['edition','country_noc','event']], on=['edition','country_noc'], how='left')\n",
    "df_model3.rename({'event':'event_count'}, axis=1, inplace=True)\n",
    "df_model3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#has country won medals in the past\n",
    "\n",
    "prior_medals = pd.DataFrame(df_medals.groupby('country_noc')['total'].sum())\n",
    "prior_medals.reset_index(inplace=True)\n",
    "prior_medals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dummy\n",
    "prior_medals['prior_medals'] = (prior_medals['total'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78363809",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_medals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using 2020 data only\n",
    "df_2020 = df_model3.loc[df_model3['year']==2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9921fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020.drop(['year', 'edition', 'gold', 'silver', 'bronze'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29171d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add medals in prior games \n",
    "\n",
    "df_medals_2016= df_medals.loc[df_medals['year']==2016]\n",
    "df_medals_2016.rename({'total':'2016_medals'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d506c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "df_2020_base = pd.merge(df_2020, df_medals_2016[['country_noc', '2016_medals']], how ='left', on=['country_noc'])\n",
    "df_2020_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12896b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_base['2016_medals'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ac1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_base.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6723eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_base.rename({'total':'total_medals'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_base = pd.merge(df_2020_base, prior_medals[['country_noc', 'prior_medals']], \n",
    "                        on='country_noc', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd73545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run a basic linear reg model\n",
    "X = df_2020_base[['is_host', 'mean_gdp_last_10yr_bn', 'mean_pop_last_10yr_mm', 'contingent_size',\n",
    "            'event_count', '2016_medals', 'prior_medals']]\n",
    "y = df_2020_base['total_medals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    X, y, test_size=0.25, random_state=101) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79860e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LinearRegression()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_predictions = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68196fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = mean_squared_error(y_test, base_predictions)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_predictions = base_model.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, base_train_predictions)\n",
    "print(mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc938a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add predictions to df \n",
    "df_2020_base['predicted_total_medals'] = base_model.predict(X)\n",
    "df_2020_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb711522",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = base_model.coef_\n",
    "\n",
    "# coefficients with their corresponding feature names\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(y_test, base_predictions, title='Residual Plot Base 2020 LR Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95821c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_results(model='Base 2020 LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2- exlcuding USA, just testing if there is any improvement\n",
    "\n",
    "\n",
    "df_2020_wo_usa = df_2020_base.loc[df_2020_base['country_noc']!= 'USA']\n",
    "\n",
    "X = df_2020_wo_usa[['is_host', 'mean_gdp_last_10yr_bn', 'mean_pop_last_10yr_mm', 'contingent_size',\n",
    "            'event_count', '2016_medals', 'prior_medals']]\n",
    "y = df_2020_wo_usa['total_medals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    X, y, test_size=0.25, random_state=101) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcf768",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LinearRegression()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_predictions = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = mean_squared_error(y_test, base_predictions)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_predictions = base_model.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, base_train_predictions)\n",
    "print(mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26407766",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(y_test, base_predictions, title='Residual Plot Base 2020 LR Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_results(model='Base 2020 LR no USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa45f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = base_model.coef_\n",
    "\n",
    "# coefficients with their corresponding feature names\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Trying with all the years instead of just 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the season dummy \n",
    "df_model3['winter'] = [1 if 'Winter' in edition else 0 for edition in df_model3['edition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging \n",
    "df_model3 = df_model3.merge(prior_medals[['country_noc', 'prior_medals']], on='country_noc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3['previous_year_medals']=df_model3.groupby(['country_noc','winter'])['total'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4850d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3['previous_year_medals'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e682b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3.drop(['edition', 'gold', 'silver', 'bronze'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3[df_model3.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e110bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_up_to_date = df_model3.groupby(['country_noc', 'winter'], as_index=False)['total'].rolling(window=3,  closed= \"left\").mean()\n",
    "mean_total_up_to_date = mean_total_up_to_date.rename(columns={'total':'3_lags_mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3 = pd.concat([df_model3, mean_total_up_to_date['3_lags_mean']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc522593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model3[['year','is_host', 'mean_gdp_last_10yr_bn', 'mean_pop_last_10yr_mm', 'contingent_size',\n",
    "            'event_count', 'previous_year_medals', 'prior_medals',\n",
    "              'winter']]\n",
    "\n",
    "y = df_model3['total']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    X, y, test_size=0.25, random_state=101, stratify=df_model3['year']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5171ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fit(X_train, y_train)\n",
    "base_predictions = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf912b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = mean_squared_error(y_test, base_predictions)\n",
    "print(np.sqrt(mse_test), mse_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63afda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_predictions = base_model.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, base_train_predictions)\n",
    "print(np.sqrt(mse_train), mse_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c834e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = base_model.coef_\n",
    "\n",
    "# coefficients with their corresponding feature names\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(y_test, base_predictions, title='Residual Plot Test All Years Model')\n",
    "plot_residuals(y_train, base_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c67aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_results(model='Base all years LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322c75b",
   "metadata": {},
   "source": [
    "<strong> Evaluation </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c908f7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The base LR model using 2020 data performs best in terms of MSE, especially when dropping the US as a country. The magnitude of the errors is lower, and the number of medals predicted go from being off by 6.5 to 6.3 (decimals are a result of using a linear regression model on count data). However, the difference between the train and test MSEs in the second fitting that drops the USA is larger. Since such a significant outlier was removed, the model is perhaps overfitting on noise that it can now better perceive. Some collinearity between variables like contingent size and event count, prior medals and previous year medals, etc. could also contribute. The R2 for the fitting without the USA is lower than that of the first, possibly because removing a row of data in a small dataset can cause information loss that affects the overall goodness of fit. Meanwhile, the base model applied to all years performs the worst, with a larger difference in train-test scores and generally worse metrics. \n",
    "\n",
    "Considering that the assumptions of linear regression are violated (e.g. heteroscedasticity, independence of observations, possible collinearity, etc.), this base model performs well. Going forward, it will be important to address the time component of the data (whether it means focusing on a smaller dataset or using all years) and the treatment of outliers like the USA (e.g., taking the log may improve the model). Moreover, since the model performs well initially, the possibilty of information leakage exists; the model may be capable of inferring a certain year's medal count based on the medal counts available for that year in the training data. \n",
    "\n",
    "Data size is another aspect: if only one year is used, the data shrinks to around 80 rows, which is not a robust number. We may need to explore data generation/oversampling techniques, or find a time-series appropriate solution that allows the use of more years. In terms of modelling, we look forward to exploring methods for count data, such as random forest, gradient boosting, poisson regression, etc. \n",
    "\n",
    "This base model furnishes a first glimpse of the feature importances. Being a host increases the medal tally by 1.5, all things equal, while winter seems to decrease it by 1.1. Previous year medals and contingent size also have a somewhat smaller, positive contribution. Applied to 2020 data, the important features become the number of events partaken in. Interestingly, the sign switches for 2016_medals after removing the USA; they are decreasing the the medals won by a country in 2020. We are currently working on adding more variables, for example related to climate, pollution, distance to the host country, etc. These are country inherent characteristics that are likely to impact the performance of athletes and in turn, the total medals. \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a492d0",
   "metadata": {},
   "source": [
    "<strong>Ongoing Work:</strong>\n",
    "\n",
    "We are going to add additional data elements like Greenhouse emissions, incentives for winning a medal, temperature to see if they improve the predictive power of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pollution variable (greenhouse gas emissions) \n",
    "ghg_df = pd.read_excel('data/raw/GHG.xlsx', sheet_name='GHG_per_capita_by_country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_df.drop(columns=['EDGAR Country Code','Country'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75450537",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_df = ghg_df[ghg_df['country_noc'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_df = pd.melt(ghg_df, id_vars=['country_noc'], var_name='year', value_name='ghg_per_capita')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_df['year'] = ghg_df['year'].astype(int)\n",
    "ghg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfe7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in temperature data\n",
    "temp_df = pd.read_csv('data/raw/temperature.csv')\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9eef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in olympic incentives\n",
    "incentives_df = pd.read_csv('data/raw/medal_incentives.csv')\n",
    "incentives_df.rename({'Gold': 'incentive_gold'}, axis=1, inplace=True)\n",
    "incentives_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#melting gdp \n",
    "\n",
    "df_country3 = df_country2.drop(['country', 'Country Name', 'Country Code', 'mean_gdp_last_10yr_bn'], axis=1)\n",
    "df_country3 = pd.melt(df_country3, id_vars=['country_noc'], var_name='year', value_name='gdp')\n",
    "df_country3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71075e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries with missing data we can still add (that arent nonsensical):\n",
    "# eri, gam, gbs, gdr, cub, kgz, lao, lbn, lca, bhu, afg, asa, bah, cgo, civ, cod,mda, tga, tkm, tur, ven,tan, \n",
    "# yem,syr, scg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country3.year = df_country3.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736445ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#melt population\n",
    "\n",
    "df_pop3 = df_pop2.drop(['country', 'Country_Updated', 'Country Code', 'mean_pop_last_10yr_mm'], axis=1)\n",
    "df_pop3 = pd.melt(df_pop3, id_vars=['country_noc'], var_name='year', value_name='population')\n",
    "df_pop3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6777405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop3.year = df_pop3.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4037d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model3 = df_model3.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb418d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with df_model3\n",
    "\n",
    "df_model4 = pd.merge(df_model3, incentives_df[['country_noc', 'offers_incentive', 'incentive_gold']], \n",
    "                          on='country_noc', how='left')\n",
    "\n",
    "df_model4.offers_incentive = df_model4.offers_incentive.fillna(0)\n",
    "df_model4.incentive_gold = df_model4.incentive_gold.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4.offers_incentive = df_model4.offers_incentive.map({'Yes': 1, 'No': 0})\n",
    "df_model4.offers_incentive = df_model4.offers_incentive.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now merge temp\n",
    "\n",
    "df_model4 = pd.merge(df_model4, temp_df[['country_noc','avg_temp']], on='country_noc', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4.avg_temp = df_model4.avg_temp.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f94fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute mean for missing temp values \n",
    "mean_temp = df_model4['avg_temp'].mean()\n",
    "df_model4['avg_temp'].fillna(mean_temp, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with emissions\n",
    "\n",
    "df_model4 = pd.merge(df_model4, ghg_df[['country_noc', 'year', 'ghg_per_capita'\n",
    "                                     ]], on=['country_noc', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a178963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4.dropna(subset=['ghg_per_capita', 'mean_pop_last_10yr_mm','mean_gdp_last_10yr_bn'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca518459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop mean gdp and pop\n",
    "df_model4.drop(['mean_pop_last_10yr_mm', 'mean_gdp_last_10yr_bn'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with gdp data\n",
    " \n",
    "df_model4 = pd.merge(df_model4, df_country3[['country_noc', 'year', 'gdp']], on=['country_noc', 'year'], \n",
    "                     how='left')\n",
    "df_model4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4 = pd.merge(df_model4, df_pop3[['country_noc', 'year', 'population']], on=['country_noc', 'year'], \n",
    "                     how='left')\n",
    "df_model4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4[df_model4['gdp'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4.loc[(df_model4['year']==1972)&(df_model4['country_noc']=='HUN'), 'gdp'] = 50.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_gdp(year,country,amount):\n",
    "    df_model4.loc[(df_model4['year']==year)&(df_model4['country_noc']==country), 'gdp'] = amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6930fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1976,'HUN',62.79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1980,'HUN',72.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd01ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1988,'HUN',85.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1972,'SUI',352.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1976,'SUI',337.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9662683",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1992,'EST',10.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f250e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1972,'POL',142.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1976,'POL',181.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1980,'POL',200.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1988,'POL',219.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1972,'ROU',54.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1976,'ROU',83.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b766cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1980,'ROU',107.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1984,'ROU',125.62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef47d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1992,'LAT',14.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1992,'IRI',217.62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1972,'LBN',19.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1980,'LBN',12.63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba256481",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1972,'BUL',21.56) # will actually have to impute, got it from croatia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1976,'BUL',23.91) # will actually have to impute, got it from croatia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(2016,'VEN',423.21) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ffad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(2020,'VEN',157.59) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b786710",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1972,'ETH',5.94) # will actually have to impute, got it from cameroon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1980,'ETH',10.92) # will actually have to impute, got it from cameroon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1972,'MGL',1.43) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1976,'MGL',1.80) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6360187",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1980,'MGL',2.32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc86f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1992,'LTU',23.39) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f58c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1992,'ISR',125.94) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1992,'SLO',23.42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1994,'SLO',25.37) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_gdp(1992,'CRO',29.68) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4.drop(['prior_medals'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4_corr = df_model4.drop(columns=['country_noc'])\n",
    "correlation_matrix = df_model4_corr.corr()\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(style=\"white\")\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a326f86",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d805e07",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Time Series Data: Strategy, Test/Train Splitting and Cross Fold Validation</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900118e5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    <p> Time series modelling is an extensive field and goes beyond the scope of this course. However, without accounting for the time components in our data, our models will not approximate reality. Our strategy is therefore to turn this time series problem into a supervised learning one by incorporating time features that account for the correlations to prior periods. \n",
    "<br><br>To begin, we must create train and test sets. Splitting the data into train and test sets considers more caution for time series problems. \n",
    "        The baseline linear regression model was leaking data from the train set into the test set, as splitting had been done using the tradional 80/20 train/test split implementation from Scikit Learn. Information from the year of prediction was in the train set, meaning information about the Olympic Games at the time of prediction that would normally be impossible to know (since the event hasn't occured) was used. Consequently, the baseline model's good performance came from overfitting, especially considering that none of the assumptions of linear regression were met. \n",
    "To mitigate this effect, we considered 3 time series splitting techniques:<li> Rolling window: using a T (time of prediction)-1 train set to predict T (validation data). The data is split into several folds of T-1 training data and T validation data.\n",
    "    <li>Expanding/Rolling window: starting with T (time of prediction)-1 as the train set and using the previous year's validation data (T) as the train once the next prediction period (T+1) arrives. The train set for that new prediction period T+1 not just include T-1, but also T-2. The train set grows cumulatively with every fold, until T-n. </li></p>\n",
    "    We chose to use an expanding window due to a limited validation set size (x=1032), meaning only 200 observations per fold if split evenly. Moreover, historical occurences may have impact on future medal count, and it is useful to see how the model performs when predicting year-on-year in the first fold, as opposed to year-on-past in the last fold. \n",
    "<br><br> In addition, cross validation is different for time series as the test set should be kept out of the train for that given year. Sectioning the data into equally sized blocks does not work. Instead, we will compare the performance for each validation fold. Since each fold is trained on a different number of years, it provides a good cross validation benchmark.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d692c",
   "metadata": {},
   "source": [
    "## Test Split: Year 2022 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9833fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to set the year as index for time series data\n",
    "# Scikit Learn's time series splitting method also requires it to be so\n",
    "df_model4.set_index('year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model4.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to datetime format\n",
    "df_model4.index = pd.to_datetime(df_model4.index, format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the year column as the index\n",
    "df_model4['year'] = df_model4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the year column\n",
    "df_model4['year'] = df_model4['year'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6085faa",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    <li> The test (untouched) data to assess final performance is from year 2022.</li>\n",
    "    <li> Everything before 2022 is part of the training set, which is further split into train and validation using the strategy detailed above.</li>\n",
    "\n",
    "            \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features= df_model4.drop(columns=['total', 'country_noc']).columns\n",
    "\n",
    "X_val = df_model4[df_model4.index<'2022'][features]\n",
    "y_val = df_model4[df_model4.index<'2022']['total']\n",
    "\n",
    "X_test_ = df_model4[df_model4.index=='2022'][features]\n",
    "y_test_ = df_model4[df_model4.index=='2022']['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032da408",
   "metadata": {},
   "source": [
    "## Available Tools: Scikit Learn TimeSeriesSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=9 # We tried several numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the splitter. The gap parameter dictates the interval between the last train data and test data.\n",
    "# It had to be increased for reasons explained below\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, gap=45) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb072bf",
   "metadata": {},
   "source": [
    "### 1. Resulting Folds & Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdda5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X_val)):\n",
    "    train_bar = ax.barh(i, len(train_index), color='blue', label=f'Iteration {i + 1} (Train)')\n",
    "    test_bar = ax.barh(i, len(test_index), left=len(train_index), color='orange', label=f'Iteration {i + 1} (Test)')\n",
    "    \n",
    "\n",
    "legend_labels = ['Train', 'Test']\n",
    "ax.legend(labels=legend_labels)\n",
    "\n",
    "ax.set_yticks(np.arange(n_splits))\n",
    "ax.set_yticklabels([f'Iteration {i+1}' for i in range(n_splits)])\n",
    "ax.set_ylabel('Iteration')\n",
    "ax.set_xlabel('Index Count')\n",
    "plt.title('Train/Test Index for Each Train/Test Split by Iteration')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb5624",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    The graph above demonstrates that the expanding window technique worked correctly. The first iteration even has more test data than train, whereas the last fold's training data is a cumulative collection of all years prior. The test set continuously remains the same size, which introduces an issue: the number of points in our data varies between the years. Not every country participates each year, meaning a 1:1 match is not possible. As time series data is dependent on previous observations, this impacts the model and will be addressed later in the notebook. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c48fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "legend_handles=[]\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X_val)):\n",
    "    train_points = ax.scatter([i] * len(train_index), X_val.index[train_index], label=f'Iteration {i + 1} (Train)', color='blue', marker='s', s=50)\n",
    "    test_points = ax.scatter([i] * len(test_index), X_val.index[test_index], label=f'Iteration {i + 1} (Test)', color='orange', marker='s', s=50)\n",
    "\n",
    "    # do we have dates that are passing into both train and test?\n",
    "    common_points = set(X_val.index[train_index]) & set(X_val.index[test_index])\n",
    "    if common_points:\n",
    "        ax.scatter([i] * len(common_points), list(common_points), color='red', marker='o', s=50)\n",
    "\n",
    "\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='blue', markersize=10, label='Train'),\n",
    "    plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='orange', markersize=10, label='Test'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Overlapping Dates in Train and Test')\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_handles)\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(n_splits))\n",
    "ax.set_xticklabels([f'Iteration {i+1}' for i in range(n_splits)]) \n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Date')\n",
    "plt.title('Train/Test Split for Each Iteration: Years Used in Split')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f01173",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    The time series splitting that Scikit Learn does not cater to data of differing observation numbers. Though we attempted to remove any overlapping train and validation data through the gap parameter, this resulted in data loss (45 observations in this example) and overlaps that persisted in iterations 3 and 4. As seen in the baseline model, this is data leakage. Therefore, we implement the expanding window ourselves below.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We lose data points: see below\n",
    "# Out of 1032 observations, we only use 987 (45 points lost, which is the gap number)\n",
    "X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There has to be a big gap as a parameter if we dont want train data year in the test but that leads to data loss\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X_val)):\n",
    "    print(f'Train size: {X_val.index[train_index].shape[0]}')\n",
    "    print(f'Test size: {X_val.index[test_index].shape[0]}')\n",
    "    print(f'Train newest date: {X_val.index[train_index].max()}')\n",
    "    print(f'Test oldest date: {X_val.index[test_index].min()}')\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb90b1",
   "metadata": {},
   "source": [
    "### 2. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it perform?\n",
    "mse_values = []\n",
    "model_summaries = [] \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X_val)):\n",
    "    X_train, X_test = X_val.iloc[train_index, :], X_val.iloc[test_index, :]\n",
    "    y_train, y_test = y_val.iloc[train_index], y_val.iloc[test_index]\n",
    "        \n",
    "    X_train = sm.add_constant(X_train) \n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "    X_test = sm.add_constant(X_test) \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    mse = np.mean((y_test - y_pred)**2)\n",
    "    rmse = mse**0.5\n",
    "    \n",
    "    mse_train = np.mean((y_train - y_pred_train)**2)\n",
    "    rmse_train = mse_train**0.5\n",
    "\n",
    "    model_summaries.append(model.summary())\n",
    "\n",
    "    print(f'Iteration {i + 1} - MSE: {mse:.4f} - RMSE: {rmse:.4f} - Train MSE: {mse_train:.4f} - Train RMSE: {rmse_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summaries[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95540d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    The performance is good with a high R-squared of 0.864 and a low average MSE (the RMSE indicates the model is only 3.6 medals off at one point). However, the splitting will cause overfitting and therefore can't be used.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab50e24",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Splitting Implementation & Fitting a Linear Regression</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67917d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    We start by determining a strategy that allows us to manually implement the expanding window. We assess the performance with linear regression and identify next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a97165",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = X_val.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22061c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every 4th date is the validation set\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ad8e9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    Implementing the expanding window is not straightforward as the time between the games are not constant. A T-1 game could be 2 years ago, but also 4 or another number. Therefore we use the unique date values from our data's index and work with indices instead. \n",
    "    <li> Given that there are 20 unique years, every 4th in the sequence will be the validation set. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1 # Used as a counter for annotating the graph\n",
    "mses = []\n",
    "mses_train = []\n",
    "rmses = []\n",
    "rmses_train = []\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(y_val.index, y_val, label='Actual', color='blue', marker='o', s=20)\n",
    "plt.title('Actual vs Predicted Values for All Iterations')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Medals')\n",
    "plt.legend()\n",
    "\n",
    "for i in range(3, 20, 4): # Starting at the fourth (third index) year, until the last, in steps of 4 \n",
    "    # Define the splits by index\n",
    "    X_train = X_val[X_val.index.isin(dates[:i])]\n",
    "    X_test = X_val[X_val.index.isin([dates[i]])]\n",
    "    y_train = y_val[y_val.index.isin(dates[:i])]\n",
    "    y_test = y_val[y_val.index.isin([dates[i]])]\n",
    "    \n",
    "    # Then fit the linear regression\n",
    "    X_train = sm.add_constant(X_train) \n",
    "    X_test['const'] = 1 # Had to add this manually because add_constant was not working on X_test.\n",
    "    position = 0  \n",
    "    X_test.insert(position, 'const', X_test.pop('const')) # Inserting into front of dataframe\n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    mse = np.mean((y_test - y_pred)**2)\n",
    "    rmse = mse**0.5\n",
    "    \n",
    "    mse_train = np.mean((y_train - y_pred_train)**2)\n",
    "    rmse_train = mse_train**0.5\n",
    "    \n",
    "    mses.append(mse)\n",
    "    mses_train.append(mse_train)\n",
    "    rmses.append(rmse)\n",
    "    rmses_train.append(rmse_train)\n",
    "    \n",
    "    plt.scatter(X_test.index, y_pred, label=f'Predicted - Iteration {i}', marker='o', s=20, color='orange')\n",
    "    \n",
    "    \n",
    "    print(f'Iteration {i + 1} - MSE: {mse:.4f} - RMSE: {rmse:.4f} - MSE train: {mse_train:.4f} - RMSE train: {rmse_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = []\n",
    "results_dict.append({'model': 'Vanilla LR',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786621fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    A linear regression performs well. Though there are some issues with slightly negative predictions and decimals, the model is off by 6-7 medals on average. The two train years show a predicted point of close to 100 medals, which is no longer there the third time. In the fourth year (first test year), the model then wrongly predicts a similar value around 100. This demonstrates that outliers/values in the tails have a strong impact on predictions. The high value close to 175 medals later causes a large overprediction in the second validation set.\n",
    "    <br><br>Notably, the first validation set does not perform as well as the others, likely due to limited data. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9f6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = list(range(1, len(mses_train) + 1))\n",
    "\n",
    "# Plotting Train and Test MSEs\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(iterations,mses_train, label='Train MSE', marker='o', color='blue')\n",
    "plt.plot(iterations, mses, label='Test MSE', marker='o', color='orange')\n",
    "\n",
    "plt.title('Train and Test Mean Squared Errors over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xticks(range(1, len(iterations) + 1))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b3eea",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    The error seems to reach a good level from a fold of 3 onwards. That means T-11 periods may be a good amount of training data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52edfd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    The coefficients of the model do not seem useful at this point. Being the host has a very strong effect, all else constant, while the other coefficients have little impact. Looking at the P-values and standard errors, the coefficients are likely not reliable. Considering that the data does not follow the assumptions of linear regression, this is most probable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66cdba",
   "metadata": {},
   "source": [
    "## Examining Residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(y_test, y_pred, 'Residuals for Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63104566",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    Given the skewed nature of the data, it is unsurprising that the residuals are heteroscedastic. It will be important to address these outliers/tails in order to improve the performance of GLMs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b0502",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(model.resid, title='ACF of residual errors')\n",
    "plt.ylim(bottom=-0.25, top=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54367f18",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    Autocorrelation is a measure of the correlation between a variable and its lagged (T-n) values. A value close to 1 or -1 indicates high correlation. The first line represents the correlation with the lag itself (T & T). Most of the correlations are dismissably low. However, though minimally, some fall outside the range of comfort. While modelling, reducing the correlations to past values will be imperative in order to successfully frame the time series problem as a supervised learning one.<br><br> The Poisson distribution also assumes that an event in the future is independent of the past - we will ensure that we reduce the autocorrelation as much as possible. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc0dfb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>GLMs: Poisson and Negative Binomial</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316928b7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    This project's data exhibits qualities that are ill-suited to linear regression, such as a right-skewed dependent variable, non-continuous values, heteroscedastic residuals, etc. Our next step is to try a more accomodating GLM. Poisson is a viable alternative as it assumes count data that occurs in fixed time intervals. However, it also requires an equal mean and variance, which will have to be verified. If the variance is greater than the mean (known as overdispersion), negative binomial regression relaxes the assumption and can be used when the data has extra variability. Both models will be tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b471e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glm_models(modeltype, X_val=X_val, y_val=y_val, graph=True, splits=False):\n",
    "    '''Packages the previously used validation and plotting code as a function that can use the 3 GLM models. \n",
    "    The plotting can be turned off, and splitting the data can be turned on so that only the 5 split data folds \n",
    "    are returned. \n",
    "    Input: GLM type, validation x and y to be split for cross validation\n",
    "    Param modeltype: string of either 'Poisson' or 'Neg Binomial', otherwise linear regression will be applied\n",
    "    Returns: the model, scores, predictions\n",
    "    '''\n",
    "\n",
    "    mses = []\n",
    "    mses_train = []\n",
    "    rmses = []\n",
    "    rmses_train = []\n",
    "    \n",
    "    if graph:\n",
    "    \n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.scatter(y_val.index, y_val, label='Actual', color='blue', marker='o', s=20)\n",
    "        plt.title(f'Actual vs Predicted Values for All Iterations {modeltype}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Total Medals')\n",
    "        plt.legend()\n",
    "    \n",
    "    if splits:\n",
    "        sets_list=[]\n",
    "        for i in range(3, 20, 4):\n",
    "            X_train = X_val[X_val.index.isin(dates[:i])]\n",
    "            X_test = X_val[X_val.index.isin([dates[i]])]\n",
    "            y_train = y_val[y_val.index.isin(dates[:i])]\n",
    "            y_test = y_val[y_val.index.isin([dates[i]])]\n",
    "\n",
    "            X_train = sm.add_constant(X_train) \n",
    "            X_test['const'] = 1 # had to add this manually because add_constant was not working on X_test. No idea why.\n",
    "            position = 0  \n",
    "            X_test.insert(position, 'const', X_test.pop('const'))\n",
    "            sets_list.append([X_train, X_test, y_train, y_test])\n",
    "        return sets_list\n",
    "    \n",
    "    \n",
    "    for i in range(3, 20, 4):\n",
    "        X_train = X_val[X_val.index.isin(dates[:i])]\n",
    "        X_test = X_val[X_val.index.isin([dates[i]])]\n",
    "        y_train = y_val[y_val.index.isin(dates[:i])]\n",
    "        y_test = y_val[y_val.index.isin([dates[i]])]\n",
    "\n",
    "        X_train = sm.add_constant(X_train) \n",
    "        X_test['const'] = 1 # had to add this manually because add_constant was not working on X_test. No idea why.\n",
    "        position = 0  \n",
    "        X_test.insert(position, 'const', X_test.pop('const'))\n",
    "\n",
    "\n",
    "        if modeltype == 'Neg Binomial':\n",
    "            model= sm.GLM(y_train, X_train, family=sm.families.NegativeBinomial()).fit()\n",
    "            \n",
    "        elif modeltype == 'Poisson':\n",
    "            model = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\n",
    "        \n",
    "        else:\n",
    "            model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        rmse = mse**0.5\n",
    "\n",
    "        mse_train = mean_squared_error(y_train,y_pred_train)\n",
    "        rmse_train = mse_train**0.5\n",
    "\n",
    "        mses.append(mse)\n",
    "        mses_train.append(mse_train)\n",
    "        rmses.append(rmse)\n",
    "        rmses_train.append(rmse_train)\n",
    "        \n",
    "        if graph:\n",
    "            plt.scatter(X_test.index, y_pred, label=f'Predicted - Iteration {i}', marker='o', s=20, color='orange')\n",
    "\n",
    "        print(f'Iteration {i + 1} - MSE: {mse:.4f} - RMSE: {rmse:.4f} - MSE train: {mse_train:.4f} - RMSE train: {rmse_train:.4f}')\n",
    "        \n",
    "        \n",
    "    return model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eeb59e",
   "metadata": {},
   "source": [
    "## Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66755fab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7894d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'Vanilla Poisson',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a875d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    MSE is higher for the Poisson model; it is off by around 10 medals on average (RMSE). The extreme point for the first test year is predicted worse in this model, with an extreme prediction more than 100 medals higher than the actual value. It is likely this comes from assumption of equal mean and variance; as seen in the resiudals, the variance is not constant and will likely fluctuate around the mean a lot. When the data is relatively similar, it predicts accurately.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = list(range(1, len(mses_train) + 1))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(iterations,mses_train, label='Train MSE', marker='o', color='blue')\n",
    "plt.plot(iterations, mses, label='Test MSE', marker='o', color='orange')\n",
    "\n",
    "plt.title('Train and Test Mean Squared Errors over Iterations - Poisson Model')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xticks(range(1, len(iterations) + 1))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347df09",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "T-15 seems to be a good training time window for this model. That is when the MSE is lowest at around +-5 medals. (RMSE) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b12b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5cfbd0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The coefficients are all very low; some variables have high standard errors in comparison. We do not believe that the equal variance and mean assumption is met. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652efc62",
   "metadata": {},
   "source": [
    "## Negative Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Neg Binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28094212",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'Vanilla Neg Binomial',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3f888",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Overdispersion occurs when the variability in the data is greater than what is expected by the assumed statistical model. However, since the predictions are much worse than the previous models, the cause is unclear. Perhaps if overdispersion is severe, the model might overestimate the uncertainty in predictions, leading to wider prediction intervals and overestimations. Multicollinearity and outliers could also be a cause, yet this would equally manifest in the previous 2 models. Research suggests that this model is sensitive to scale, since the pattern of dispersion is critically dependent on it. We will test these in the coming sections.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb0fc2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Assumptions Check</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c862ab",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    In order to understand what further pre-processing needs to occur for improved model performance, we analyze the distribution of the target closely and attempt some transformations as a preliminary test. We then address the requirement of an equal mean and variance, which is closely related to the time series concept of a constant mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995a357",
   "metadata": {},
   "source": [
    "## Homoscedasticity & Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_model4['total'], bins=20, color='lightblue', edgecolor='black')\n",
    "plt.title('Histogram of Total Medals')\n",
    "plt.xlabel('Total Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(df_model4['total'], vert=False, widths=0.7, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "plt.title('Boxplot of Total Medals')\n",
    "plt.xlabel('Total Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639136f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "We see that the skew is very strongly to the right, with a long tail. The median medals are close to zero. Since we witnessed overpredictions in all models, reducing the skew, if not completely eliminating it, will prove beneficial. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2169c5",
   "metadata": {},
   "source": [
    "### 1. Removing Skew: Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129df7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the boxplot for np.log(df_model4['total'])\n",
    "axes[0].hist(np.log(df_model4['total']), bins=20, color='lightblue', edgecolor='black')\n",
    "\n",
    "axes[0].set_title('Histogram of the Log of Total Medals')\n",
    "\n",
    "# Plot the boxplot for np.sqrt(df_model4['total'])\n",
    "axes[1].boxplot(np.log(df_model4['total']), vert=False, widths=0.7, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "axes[1].set_title('Histogram of the Log of Total Medals')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b966e5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Taking the log reduces the skew and eliminates the long tail of outliers. However, the low medal count between zero and one is still pulling the distribution out of shape. This may present a problem for linear regression, which works best with a normally distributed target, yet should suffice for the other models. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae363f",
   "metadata": {},
   "source": [
    "### 2. Removing Skew: Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697eb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "import scipy.stats as stats\n",
    "\n",
    "transformed_data, lambda_value = boxcox(df_model4['total']) \n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "original_data = df_model4['total']\n",
    "axes[0].hist(original_data, bins=20, color='lightblue', edgecolor='black')\n",
    "axes[0].set_title('Histogram of Original Data')\n",
    "axes[0].set_xlabel('Total Values')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(transformed_data, bins=20, color='lightblue', edgecolor='black')\n",
    "axes[1].set_title(f'Histogram of Box-Cox Transformed Data (lambda={lambda_value:.2f})')\n",
    "axes[1].set_xlabel('Transformed Values')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig2, axes2 = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "stats.probplot(original_data, dist=stats.norm, plot=axes2[0])\n",
    "axes2[0].set_title('Probability Plot - Original Data')\n",
    "\n",
    "stats.probplot(transformed_data, dist=stats.norm, plot=axes2[1])\n",
    "axes2[1].set_title('Probability Plot - Box-Cox Transformed Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb67a0c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The Box-Cox transformation is a good choice as it is used to stabilize the variance and make a dataset approximately normal or more symmetric. It identifies a an parameter, lambda, for the transformation. This is most likely the most symmetric we can get the data without addressing the 0 values. If this seems necessary in future predictions, we will.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8647f",
   "metadata": {},
   "source": [
    "## Overdispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451599d",
   "metadata": {},
   "source": [
    "### 1. Formal Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b930ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6450a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The dispersion parameter (model.scale) in this context represents a scale or dispersion factor related to the assumed variance structure of the binomial distribution. It reflects how much the actual distribution's variance differs from the variance predicted by the model. A higher value indicates less dispersion, approaching a Poisson distribution. The value obtained from the negative binomial model suggests that though there is some overdispersion, it is not severe.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbffc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = adfuller(transformed[0])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08eadf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The AD Fuller test is used to test whether the data has a unit root, i.e. a trend component that is constant over time. The null hypothesis is that the data has a unit root. Since the p-value is below 0.05 and the statistic is quite negative (the more negative, the less constant), we can conclude that the data has non-stationarity.  </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115fbea",
   "metadata": {},
   "source": [
    "### 2. Visual Inspection: Mean and Variance, Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_mean_total_winter = df_model4[df_model4['winter']==1]['total'].resample('Y').mean()\n",
    "yearly_mean_total_summer = df_model4[df_model4['winter']==0]['total'].resample('Y').mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=yearly_mean_total_winter.index, y=yearly_mean_total_winter, marker='o', color='lightblue', label='Yearly Mean Medals: Winter')\n",
    "sns.lineplot(x=yearly_mean_total_summer.index, y=yearly_mean_total_summer, marker='o', color='orange', label='Yearly Mean Medals: Summer')\n",
    "\n",
    "plt.title('Yearly Mean of Total Medals: Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Yearly Mean Total Medals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67516e04",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Looking at the line graph of the total mean medals gives us an idea about how the variance and mean behave. In this case, a split was made between winter and summer, due to the seasonality. However, we accounted for the differences there with a seasonality variable ('winter') already, so it should not be relevant. We observe that summer has a large spike in variance in the early years, while the winter games seem more stable. Both exhibit a positive trend, which signifies the mean does not stay constant. Thus, the variance is not completely homogeneous, while the mean is growing; there is a violation of the proportional mean and variance assumption.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f50e7",
   "metadata": {},
   "source": [
    "### 3. Methods for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c74e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data, lambda_val = boxcox(df_model4['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d97c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = pd.concat([df_model4.reset_index(drop=True), pd.DataFrame(transformed_data)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed['year'] = pd.to_datetime(transformed['year'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6bf19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformed.set_index('year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_mean_total_winter = transformed[transformed['winter']==1][0].resample('Y').mean()\n",
    "yearly_mean_total_summer = transformed[transformed['winter']==0][0].resample('Y').mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=yearly_mean_total_winter.index, y=yearly_mean_total_winter, marker='o', color='lightblue', label='Yearly Mean Medals: Winter')\n",
    "sns.lineplot(x=yearly_mean_total_summer.index, y=yearly_mean_total_summer, marker='o', color='orange', label='Yearly Mean Medals: Summer')\n",
    "\n",
    "plt.title('Yearly Mean of Total Medals: Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Yearly Mean Total Medals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2aa603",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "We perform a Box-Cox transformation that should stabilize the variance. The variance looks a little better, and the Box-Cox transformation can also have an impact on the mean of the data, especially if the original data.\n",
    "In some cases, the transformation may reduce the influence of extreme values (outliers), potentially leading to a more consistent mean, which is why the trend looks more stable. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller(df_model4['total'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller(df_model4['total'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20367e38",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Since the AD Fuller statistic became less negative, we can conclude that this transformation imrpoved the mean a little. \n",
    "As the variance also looks more stable, the assumption of equal mean and variance will be met more easily.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_mean_total = transformed[0].resample('Y').mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=yearly_mean_total.index, y=yearly_mean_total, marker='o', color='red', label='Yearly Mean Medals: All Games')\n",
    "\n",
    "plt.title('Yearly Mean of Total Medals: Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Yearly Mean Total Medals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5360c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "It seems the seasonality was treated well enough through its dummy variable: there is no immediately apparent seasonal fluctuation in the mean total medals. The variance is more constant, though there are peaks in the later years. The trend is not as extreme as the initial, untransformed data, yet it is still there. We believe that applying a Box-Cox transformation will help with the equal mean-variance assumption, but not bypass it. As further treatment would require more time series techniques, we will not go beyond this. Instead, we will check for GLM performance improvement as consequence of the transformation, then try non-parametric approaches. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1896101",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Exploring Scales</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0af36c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    The target variable is not the only one with a problematic distribution. As we confirmed during EDA, most predictors have a skew and outliers. In order to understand their magnitude and treat them accordingly, we will compare their scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec78eb",
   "metadata": {},
   "source": [
    "## Differences in Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where do we need to adjust the scale?\n",
    "X_val.plot(kind='box', figsize=(12, 6), grid=True)\n",
    "plt.title('Boxplot of All Numerical Predictors')\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2777828",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The incentive for gold has very high numbers, though it is actually not justified: GDP is higher, yet is in billions. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdp, year\n",
    "X_val[[col for col in X_val.columns if 'incentive_gold' not in col]].plot(kind='box', figsize=(12, 6), grid=True)\n",
    "plt.title('Boxplot of Numerical Values Excluding incetive_gold')\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93c54f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Without the incentive for gold, we observe the true scale perturber: GDP. This makes sense as GDP is a large sum. Year is also a culprit: it is interpreted as a number in units of thousands. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ebb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdp, year\n",
    "X_val[[col for col in X_val.columns if 'incentive_gold' not in col and 'gdp' not in col and col!='year']].plot(kind='box', figsize=(12, 6), grid=True)\n",
    "plt.title('Boxplot of Numerical Values Excluding incentive_gold, gdp, year')\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b77160",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Since scale issues persist even after addressing the largest perturbers, we will add a scaler to our pre-processing pipeline.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64dff73",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Exploring Predictor Transformations</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6553c6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    In this section, we investigate the distributions of the predictors to identify whether a transformation is necessary. If possible, we wish to avoid transforming and use scaling instead. Transformations make interpretation more difficult than scaling and can be more cumbersome to undo. We also wish to preserve the original distribution of predictors as much as possible. Nevertheless, we will explore them here in case the need arises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21031134",
   "metadata": {},
   "source": [
    "### 1. Original Predictor Distirbutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 14))\n",
    "for i, column in enumerate(['contingent_size', 'event_count', 'previous_year_medals', 'incentive_gold', 'avg_temp', 'ghg_per_capita', 'gdp',\n",
    "      'population', 'previous_year_medals', '3_lags_mean'], 1):\n",
    "    plt.subplot(3, 4, i) \n",
    "    sns.histplot(X_val[column], kde=True,  label=column)\n",
    "    plt.title(f'Histogram with KDE - {column}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.suptitle('Histograms with KDEs for Numerical Columns, No Transformation', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf57e90",
   "metadata": {},
   "source": [
    "### 2. Square Root Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 14))\n",
    "for i, column in enumerate(['contingent_size', 'event_count', 'previous_year_medals', 'incentive_gold', 'avg_temp', 'ghg_per_capita', 'gdp',\n",
    "      'population', 'previous_year_medals', '3_lags_mean'], 1):\n",
    "    plt.subplot(3, 4, i)  \n",
    "    sns.histplot(np.sqrt(X_val[column]), kde=True,  label=column, color='green')\n",
    "    plt.title(f'Histogram with KDE - {column}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.suptitle('Histograms with KDEs for Log Transformed Columns', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccab98e",
   "metadata": {},
   "source": [
    "### 3. Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24396012",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 14))\n",
    "for i, column in enumerate(['contingent_size', 'event_count', 'previous_year_medals', 'incentive_gold', 'avg_temp', 'ghg_per_capita', 'gdp',\n",
    "      'population', 'previous_year_medals', '3_lags_mean'], 1):\n",
    "    plt.subplot(3, 4, i)  \n",
    "    sns.histplot(np.log(X_val[column]), kde=True,  label=column, color='red')\n",
    "    plt.title(f'Histogram with KDE - {column}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.suptitle('Histograms with KDEs for Log Transformed Columns', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3146fefd",
   "metadata": {},
   "source": [
    "### 4. Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "for i, column in enumerate(['contingent_size', 'event_count', 'previous_year_medals', 'incentive_gold', 'avg_temp', 'ghg_per_capita', 'gdp',\n",
    "      'population', 'previous_year_medals', '3_lags_mean'], 1):\n",
    "    plt.subplot(3, 4, i)\n",
    "    transformed_data, lambda_value = boxcox(X_val[column]+1) # it is saying we have negative values even though we dont\n",
    "\n",
    "    \n",
    "    sns.histplot(transformed_data, kde=True, label=f'{column} (Box-Cox)', color='purple')\n",
    "    plt.title(f'Histogram with KDE - {column} (Box-Cox)')\n",
    "    plt.legend()\n",
    "\n",
    "plt.suptitle('Histograms of Numerical Columns, Box-Cox Transformed', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22237d0b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The most gentle transformation is the square root. While it reduces the extreme skew that most predictors exhibit, it does not alter the distribution shape drastically. In contrast, the log transformation has the greatest impact and will even move the skew to the other side. The Box-Cox is better at creating the most normal-approximation, yet also seems to extreme in some cases. <br><br> We may use the square root transformation. However, we will attempt scaling instead as it does not directly alter the distribution but can soften the impact of outliers/extreme values. Instead of standardizing, which would not help address the outliers, Scikit Learn's Robust Scaler can be used. Robust scaling  is less influenced by extreme values as it uses the median (rather than the mean) and interquartile range to calculate the scale. These statistics are less sensitive to outliers compared to the mean and standard deviation used in standardization. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f0fc9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Applying Transformations & Scaling</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b4ad0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    After evaluating the possibilities, the robust scaler will be used to scale the predictors, while the Box-Cox transformation is appropriate for the total medals (dependent variable). We apply both in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42adb6ff",
   "metadata": {},
   "source": [
    "## Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('rs', RobustScaler(), X_val.columns),\n",
    "    ],\n",
    ")\n",
    "\n",
    "X_val_scaled = preprocessor.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled = X_val_scaled.set_index(X_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8dac4",
   "metadata": {},
   "source": [
    "## Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf89dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data, lambda_value = boxcox(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961332ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = pd.DataFrame(transformed_data, columns=['transformed']).set_index(y_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea026e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Evaluating Performance</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43bc9f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    Having performed the changes which we believe could improve the model, we evaluate the outcome. Note that we do not just apply all changes together, but also test them individually to assess their effect more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c94996",
   "metadata": {},
   "source": [
    "## Negative Binomial Model: Scaled and Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f64968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg binomial y transformed and x scaled, not great\n",
    "\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Neg Binomial', \n",
    "                                                                               y_val=transformed_data['transformed'], \n",
    "                                                                               X_val=X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54caa240",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'S+T Neg Binomial',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48512812",
   "metadata": {},
   "source": [
    "## Negative Binomial Model: Scaled Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8bd69c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# neg binomial x scaled, not great\n",
    "\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Neg Binomial', \n",
    "                                                                               y_val=y_val, \n",
    "                                                                               X_val=X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4aeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'S Neg Binomial',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2ec0c",
   "metadata": {},
   "source": [
    "## Negative Binomial Model: Transformed Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg binomial x scaled, not great\n",
    "\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Neg Binomial', \n",
    "                                                                               y_val=transformed_data['transformed'], \n",
    "                                                                               X_val=X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'T Neg Binomial',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e973e",
   "metadata": {},
   "source": [
    "## Poisson Model: Scaled & Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poisson transformed total + scaled predictors: the worst\n",
    "\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Poisson', \n",
    "                                                                               y_val=transformed_data['transformed'], \n",
    "                                                                               X_val=X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ec6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'S+T Poisson',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff50c5",
   "metadata": {},
   "source": [
    "## Poisson Model: Scaled Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poisson scaled X, good \n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Poisson', \n",
    "                                                                               y_val=y_val,\n",
    "                                                                               X_val=X_val_scaled\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'S Poisson',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28344c",
   "metadata": {},
   "source": [
    "## Linear Regression Model: Transformed & Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d36406",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# not good\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models(modeltype='LR',y_val=transformed_data['transformed'],\n",
    "                                                                               X_val=X_val_scaled\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'S+T Linear Regression',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b08715",
   "metadata": {},
   "source": [
    "## Linear Regression Model: Scaling Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee941ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ok but the initial model was better\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models(modeltype='LR',\n",
    "                                                                               X_val=X_val_scaled\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2fe69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'S Linear Regression',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee276cf1",
   "metadata": {},
   "source": [
    "## Linear Regression Model: Transformed Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dad5f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bad\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models(modeltype='LR',y_val=transformed_data['transformed'], \n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e01bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'T Linear Regression',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ab133",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Surprisingly, the Box-Cox transformation of total medals did not benefit any model. The reason for this seems to be that it changes the original distribution too much, as was the case when the predictors were transformed. Instead, a square root transformation provides better results. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856328f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less overpredictions than initially \n",
    "\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Neg Binomial', \n",
    "                                                                               y_val=np.sqrt(y_val), \n",
    "                                                                               X_val=X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'Sqrt T Neg Binomial',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('Poisson', \n",
    "                                                                               y_val=np.sqrt(y_val), \n",
    "                                                                               X_val=X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'Sqrt T Poisson',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models('lr', \n",
    "                                                                               y_val=np.sqrt(y_val), \n",
    "                                                                               X_val=X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'Sqrt T Linear Regression',\n",
    "              'average_mse': np.mean(mses),\n",
    "              'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    " split = glm_models('lr', y_val=np.sqrt(y_val), X_val=X_val,graph=False, splits=True)[4]\n",
    "                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = pd.DataFrame(split[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "untransformed_2020 = pd.concat([actual, pd.DataFrame(y_pred)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "untransformed_2020[list(untransformed_2020.columns)] = untransformed_2020[list(untransformed_2020.columns)]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(untransformed_2020['total'],untransformed_2020[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1330fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "30.182397117706586**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ae0cd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "For the negative binomial model, the performance improves, but is still not comparable with the Poisson and Linear regression ones. Even the linear regression model does not improve compared to the baseline, despite the skewed nature of the dependent variable. It seems the original distibution is important, but the outliers are the problem.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99edd9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Engineering Interaction Terms</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e4e18",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    Since one of the assumptions of linear regression is no multicollinearity, but time variables are often very correlated, interactions may be helpful. In order to test this hypothesis, we first check whether one interaction (between the most correlated variables contingent_per_event and contingent_size) makes a difference.\n",
    "    Once this is confirmed, we create interaction terms for all variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c4e639",
   "metadata": {},
   "source": [
    "## Testing One Interaction Term for All GLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47df373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations: contingent size x event count \n",
    "\n",
    "def add_interactions(X):\n",
    "    X['contingent_per_event'] = X['contingent_size']/X['event_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f059d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "add_interactions(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.transformers[0] = ('rs', RobustScaler(), X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled = preprocessor.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eccb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very similar\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models(modeltype='LR',\n",
    "                                                                               X_val=X_val\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#improved a lot\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models(modeltype='Poisson',\n",
    "                                                                               X_val=X_val\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did improve but still bad\n",
    "model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models(modeltype='Neg Binomial',\n",
    "                                                                               X_val=X_val\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afd7e3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "As all models showed a favorable or neutral outcome, we will pursue the interactions further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b90d7f",
   "metadata": {},
   "source": [
    "## Adding all Possible Interaction Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.drop(columns='contingent_per_event', inplace=True)\n",
    "X_val_scaled.drop(columns='contingent_per_event', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2479cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.transformers[0] = ('rs', RobustScaler(), X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "interactions = poly.fit_transform(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d027ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.DataFrame(interactions, columns=poly.get_feature_names_out(X_val_scaled.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188527e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.set_index(X_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edb7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ['LR', 'Poisson', 'Neg Binomial']:\n",
    "    print(f'------------------------------------------{m}-----------------------------------------')\n",
    "    model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train = glm_models(modeltype=m,\n",
    "                                                                               X_val=interactions,\n",
    "                                                                               graph=False)\n",
    "    print('---------------------------------------------------------------------------------------')\n",
    "    results_dict.append({'model': f'{m} Interactions',\n",
    "          'average_mse': np.mean(mses),\n",
    "          'average_rmse': np.mean(rmses)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03826a29",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Lasso Regression</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1114e1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    With all interactions, we are guaranteed to have overfitting. We therefore want to choose the variables that are actually important and get rid of the others, which Lasso regularization does. In order to do so optimally, we need to identify the best penalty value for alpha. We pick a large range of alphas between -4 and 4 on a log scale, then use our cross-validation folds to pick the alpha that minimizes the MSE for all folds the most on average. Once this initial value is found, we change the logscale to values that are close to the chosen alpha for tuning. Finally, we fit the Lasso regression with the optimal lambda and evaluate the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19c76e",
   "metadata": {},
   "source": [
    "## Identifying a Feasible Range of Values for the Optimal Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_cv(start=-4, end=4, num=20):\n",
    "    '''Cross validation for the optimal penalty.\n",
    "    Input: Start and end of the alpha search space, and the number of alpha values that should be tested\n",
    "    Output: Each alpha and their mean MSE scores based on the cross validation folds\n",
    "    '''\n",
    "    grid = np.logspace(start, end, num)\n",
    "    results = []\n",
    "    results_train = []\n",
    "    for alpha in grid:\n",
    "        mses=[]\n",
    "        mses_train=[]\n",
    "        sets = glm_models(modeltype='LR',X_val=interactions,graph=False, splits=True)\n",
    "\n",
    "        for i in range(5):\n",
    "            X_train, X_test, y_train, y_test = sets[i]\n",
    "            lasso = Lasso(alpha=alpha, fit_intercept=False,random_state=1)\n",
    "            lasso.fit(X_train,y_train)\n",
    "            mse = mean_squared_error(y_test, lasso.predict(X_test))\n",
    "            mses.append(mse)\n",
    "\n",
    "            mse_train = mean_squared_error(y_train, lasso.predict(X_train))\n",
    "            mses_train.append(mse_train)\n",
    "\n",
    "        results.append({'alpha': alpha,\n",
    "                        'mse': np.mean(mses)})\n",
    "        results_train.append({'alpha': alpha,\n",
    "                        'mse_train': np.mean(mses_train)})\n",
    "        \n",
    "    return results, results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5623c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, results_train = lasso_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f18b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = pd.DataFrame(results_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mse_lambda = results['mse'].min()\n",
    "optimal_lambda = results.loc[results['mse'].idxmin()]['alpha']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results['alpha'], results['mse'], marker='o', linestyle='-', color='b', label='Test MSE')\n",
    "plt.plot(results['alpha'], results_train['mse_train'], marker='o', linestyle='-', color='r', label='Train MSE')\n",
    "plt.axvline(x=optimal_lambda, color='gray', linestyle='--', label=f'Optimal Lambda: {optimal_lambda:.3f}')\n",
    "plt.xscale('log')  \n",
    "plt.title('Lambda vs. MSE')\n",
    "plt.xlabel('Lambda (log scale)')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c50fc1",
   "metadata": {},
   "source": [
    "## Fitting the Model on Each Validation Fold Using the Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db0bc9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "In order to compare how successful our subsequent tuning attempts are, we check the model's performance using the current alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d2538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mses=[]\n",
    "sets = glm_models(modeltype='LR',X_val=interactions,graph=False, splits=True)\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = sets[i]\n",
    "\n",
    "    lasso_model = Lasso(alpha=optimal_lambda, fit_intercept=False, random_state=1)\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test,lasso_model.predict(X_test))\n",
    "    mses.append(mse)\n",
    "    print(f'----------------Fold {i+1}----------------)')\n",
    "    print(f'-- mse: {mse}')\n",
    "    print(f'-- rmse: {mse**0.5}')\n",
    "    print('------------------------------------------)')\n",
    "\n",
    "results_dict.append({'model': 'Untuned Lasso',\n",
    "          'average_mse': np.mean(mses),\n",
    "          'average_rmse': np.mean(np.mean(mses)**0.5)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mses)**0.5 #rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e404977",
   "metadata": {},
   "source": [
    "## Tuning the Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, results_train = lasso_cv(start=-1, end=1.5, num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3318b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f726b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = pd.DataFrame(results_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mse_lambda = results['mse'].min()\n",
    "optimal_lambda = results.loc[results['mse'].idxmin()]['alpha']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results['alpha'], results['mse'], marker='o', linestyle='-', color='b', label='Test MSE')\n",
    "plt.plot(results['alpha'], results_train['mse_train'], marker='o', linestyle='-', color='r', label='Train MSE')\n",
    "plt.axvline(x=optimal_lambda, color='gray', linestyle='--', label=f'Optimal Lambda: {optimal_lambda:.3f}')\n",
    "plt.xscale('log')  \n",
    "plt.title('Lambda vs. MSE')\n",
    "plt.xlabel('Lambda (log scale)')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211825f",
   "metadata": {},
   "source": [
    "## Fitting the Model on all Validation Folds Again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = glm_models(modeltype='LR',X_val=interactions,graph=False, splits=True)\n",
    "mses=[]\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = sets[i]\n",
    "\n",
    "    lasso_model = Lasso(alpha=optimal_lambda, fit_intercept=False, random_state=1)\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test,lasso_model.predict(X_test))\n",
    "    mses.append(mse)\n",
    "    print(f'mse: {mse}')\n",
    "    print(f'rmse: {mse**0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'Tuned Lasso',\n",
    "          'average_mse': np.mean(mses),\n",
    "          'average_rmse': np.mean(np.mean(mses)**0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb30d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mses)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f09a6a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "We see an improvement from the initial Lasso model with rough tuning to the new, optimally tuned one. The average MSE improved from 65.82 to 64.54."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict).sort_values(by='average_rmse', ascending=False).query('model == \"Untuned Lasso\" or model == \"Tuned Lasso\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d6ea8",
   "metadata": {},
   "source": [
    "## Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0d158",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "Which features did the lasso model keep? Now that we applied L1 regularization, we are left with the variables the lasso regression deems as truly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = list(zip(interactions.columns,lasso_model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = [(feature, coefficient) for feature, coefficient in coeffs if coefficient > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200db8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coeffs).sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_coeffs = set([feature.split()[0] for feature, coefficient in coeffs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ea976",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_coeffs.update(set([feature.split()[-1] for feature, coefficient in coeffs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the single variables without interactions\n",
    "regular_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e72191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and here we have them all together\n",
    "# 33 predictors\n",
    "# Counting the base variables that werent included as important in the model (e.g. if an interaction is important, \n",
    "# We include both base variables)\n",
    "initial_variables = set(list(regular_coeffs) + [feature for feature, coefficient in coeffs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef484a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Bootstrapping Lasso Regression</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66cb97",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    Though our lasso regression determined certain features to be important, we already ascertained that the assumptions of linear regression are not met. Though we scaled the data as is required for lasso, that is not sufficient for a reliable model. We therefore use bootstrapping to generate the final variable importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbdc52a",
   "metadata": {},
   "source": [
    "## Obtaining Robust Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeffs(model,X_val, features=True):\n",
    "    '''Gets the lasso regression coefficients greater than zero and returns their names as a list. \n",
    "    If features is set to False, a list of tuples with the feature name and coefficient is returned.\n",
    "    '''\n",
    "    \n",
    "    coeffs = list(zip(X_val.columns,model.coef_))\n",
    "    coeffs = [(feature, coefficient) for feature, coefficient in coeffs if coefficient > 0]\n",
    "    \n",
    "    if not features: \n",
    "        return coeffs\n",
    "    \n",
    "    regular_coeffs = set([feature.split()[0] for feature, coefficient in coeffs])\n",
    "    regular_coeffs.update(set([feature.split()[1] for feature, coefficient in coeffs]))\n",
    "    return list(regular_coeffs) + [feature for feature, coefficient in coeffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef27e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(df,num_samples=30):\n",
    "    '''Bootstraps the inputted dataframe n (num_samples) times. Resulting dataframes are returned in a list'''\n",
    "    bootstraps=[]\n",
    "    for _ in range(num_samples+1):\n",
    "        bootstrap_sample = df.sample(frac=1, replace=True)\n",
    "        bootstraps.append(bootstrap_sample)\n",
    "    return bootstraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = glm_models(modeltype='LR', X_val=interactions, graph=False, splits=True) # getting our cross validation sets\n",
    "\n",
    "coeffs = {}\n",
    "for i in range(5): # For each fold we will conduct bootstrapping of the trianing data\n",
    "    X_train, X_test, y_train, y_test = sets[i]\n",
    "    df = pd.concat([X_train, y_train], axis=1) # Concatenating to bootstrap\n",
    "    mses = []\n",
    "    mses_train = []\n",
    "    for bootstrap_sample in bootstrap(df=df, num_samples=200): # 200 bootstraps for robustness\n",
    "        X_train_bootstrap = bootstrap_sample.drop('total', axis=1)\n",
    "        y_train_bootstrap = bootstrap_sample['total']\n",
    "        lasso_model = Lasso(alpha=optimal_lambda, fit_intercept=False, random_state=1)\n",
    "        lasso_model.fit(X_train_bootstrap, y_train_bootstrap) # Fit model on bootstrap\n",
    "        mse = mean_squared_error(y_test, lasso_model.predict(X_test))\n",
    "        mse_train = mean_squared_error(y_train_bootstrap, lasso_model.predict(X_train_bootstrap))\n",
    "        mses.append(mse)\n",
    "        mses_train.append(mse_train)\n",
    "        \n",
    "        for var,coeff in get_coeffs(lasso_model, features=False, X_val=X_train):  # Get list of feature,coefficient tuples\n",
    "            if var not in coeffs: # Add to coefficient dictionary if the feature name isnt in there yet\n",
    "                coeffs[var] = [coeff]\n",
    "            else:\n",
    "                coeffs[var].append(coeff) \n",
    "        # At this point we have a dictionary with feature names and all the lists of coefficients generated for them\n",
    "\n",
    "    print(f'-------------------------Fold {i+1}----------------------------')\n",
    "    print(f' -- mse : {np.mean(mses)}  -- train mse: {np.mean(mses_train)}')\n",
    "    print(f' -- rmse : {np.mean(mses)**0.5} -- train rmse: {np.mean(mses_train)**0.5}')\n",
    "    print(f'----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148eed87",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "200 random bootstraps of the training data were made for each cross validation fold. Considering this, the performance of the model is very robust, with an rmse of 5 in the last fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416674e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable, values in coeffs.items():\n",
    "    if values:  # Check if the list is not empty to avoid division by zero\n",
    "        mean_value = sum(values) / len(values)  # Get mean value of feature\n",
    "        coeffs[variable] = mean_value\n",
    "    else:\n",
    "        coeffs[variable] = None # Handle 0 coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79daa43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_coeffs=set()\n",
    "for col in [i.split() for i in list(coeffs.keys())]:\n",
    "    regular_coeffs.add(col[0])\n",
    "    regular_coeffs.add(col[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c347947",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_coeffs = list(regular_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74438207",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pd.DataFrame([coeffs]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.columns = ['coefficient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1810c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = coeffs.sort_values(by='coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53cb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = coeffs.drop('const')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.index = coeffs.index.map(lambda x: x.replace(' ', ' x '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abbad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.shape # 70-80 predictors are usually important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = coeffs.sort_values(by='coefficient') # Necessary for the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351593e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "\n",
    "plt.figure(figsize=(10, 25))\n",
    "bars = plt.barh(coeffs.index, coeffs['coefficient'], color='skyblue', height=0.9) \n",
    "\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Bootstrapped Coefficient Importance using Lasso Regression (n Boostraps = 200)')\n",
    "plt.grid(False) \n",
    "\n",
    "for bar in bars:\n",
    "    xval = bar.get_width()\n",
    "    plt.text(xval, bar.get_y() + bar.get_height()/2, round(xval, 3), va='center')\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da981d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bootstrapped = set(coeffs.index.map(lambda x: x.replace(' x ', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23075dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bootstrapped.update(regular_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab735d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_variables-vars_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd636dea",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "    <li> Note: the effects described below will very with each re-run of the notebook, yet the general effects are similar <br><br>\n",
    "Above, we compare the set of initial variables from the lasso regression without bootstrapping to the bootstrapped ones. The difference are those that were in the initial set but not in the bootstrapped results. The single variables are not relevant as they are used in interactions and must therefore be included in the model anyways. <br> <li> is host x offers_incentive is not included, which is reasonable given its previously low coefficient. <li> contingent_size x winter is not included, which may be surprising given its initial importance. However, contingent_size x avg_temp is now important: it seems these two interactions are correlated and therefore have a cancelling effect on another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = glm_models(modeltype='LR',X_val=interactions,graph=False, splits=True)\n",
    "mses=[]\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = sets[i]\n",
    "\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_train[list(vars_bootstrapped)], y_train)\n",
    "    mse = mean_squared_error(y_test,model.predict(X_test[list(vars_bootstrapped)]))\n",
    "    mses.append(mse)\n",
    "    print(f'mse: {mse}')\n",
    "    print(f'rmse: {mse**0.5}')\n",
    "results_dict.append({'model': 'Bootstrapped Features LR',\n",
    "      'average_mse': np.mean(mses[-3:]),\n",
    "      'average_rmse': np.mean(np.mean(mses[-3:])**0.5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa5768",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "The MSE is very high for the initial folds, but decreases. Due to the high number of features, it is better the largest train data folds possible. We will therefore only consider the scores of the last 3 folds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3018e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "\n",
    "Using all of these variables produces a good mse. However, we must keep in mind that some very correlated coefficients make it into the list due to the high number of bootstraps. It is reasonable to remove those below 0.1 or higher. A forward, bootstrapped variable selection technique would help in this matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4666774",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Building a Voting Regressor</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b7d71",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    Unlike GLMs, ensemble methods are non-parametric and allow us to not only specify possible interactions (which we investigated in advance), but also control the overfitting that likely results from the numerous features. In order to increase generalizability and benefit from gradient boosting as well as the random forest feature selection and boostrapping, we will use a voting regressor. This combines all 3 models into one and averages their prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting regressor-fits several base regressors and then takes the average of the predictions \n",
    "\n",
    "#fitting the models\n",
    "def ensemble_models(X_train, X_test, y_train, y_test, ensemble_type, graph=True):\n",
    "    mses = []\n",
    "    mses_train = []\n",
    "    rmses = []\n",
    "    rmses_train = []\n",
    "\n",
    "    if graph:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.scatter(y_test.index, y_test, label='Actual', color='blue', marker='o', s=20)\n",
    "\n",
    "      # Create a new index for y_train and y_test\n",
    "        new_index_train = np.arange(len(y_train))\n",
    "        new_index_test = np.arange(len(y_test))\n",
    "\n",
    "        # Align indices of y_train and X_train\n",
    "        y_train.index = new_index_train\n",
    "        X_train.index = new_index_train\n",
    "\n",
    "        # Align features in X_test with X_train\n",
    "        X_test = X_test[X_train.columns]\n",
    "\n",
    "        # Check if X_test has a constant column, if not, add it\n",
    "    if 'const' not in X_test.columns:\n",
    "        X_test.insert(0, 'const', 1)\n",
    "\n",
    "\n",
    "    # Check if X_train has a constant column, if not, add it\n",
    "    if 'const' not in X_train.columns:\n",
    "        X_train = sm.add_constant(X_train)\n",
    "    \n",
    "    #Base models\n",
    "    base_models = []\n",
    "\n",
    "    if ensemble_type == 'random_forest':\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        base_models.append(('random_forest', model))\n",
    "    elif ensemble_type == 'xgboost':\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        base_models.append(('xgboost', model))\n",
    "    elif ensemble_type == 'gradient_boosting':\n",
    "        model = GradientBoostingRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        base_models.append(('gradient_boosting', model))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ensemble type. Please specify 'random_forest' or 'xgboost'.\")\n",
    "\n",
    "    # Check if y_test is not None and not empty\n",
    "    if y_test is not None and y_test.size > 0:\n",
    "        ensemble_model = VotingRegressor(estimators=base_models)\n",
    "        ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = ensemble_model.predict(X_test)\n",
    "        y_pred_train = ensemble_model.predict(X_train)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = mse**0.5\n",
    "\n",
    "        mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "        rmse_train = mse_train**0.5\n",
    "\n",
    "        mses.append(mse)\n",
    "        mses_train.append(mse_train)\n",
    "        rmses.append(rmse)\n",
    "        \n",
    "        rmses_train.append(rmse_train)\n",
    "\n",
    "        if graph:\n",
    "            plt.scatter(X_test.index, y_pred, label=f'Predicted', marker='o', s=20, color='orange')\n",
    "            plt.title(f'Actual vs Predicted Values - Ensemble Model: {ensemble_type}')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Total Medals')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        print(f'MSE: {mse:.4f} - RMSE: {rmse:.4f} - MSE train: {mse_train:.4f} - RMSE train: {rmse_train:.4f}')\n",
    "\n",
    "    return ensemble_model, mses, mses_train, rmses, rmses_train, y_pred, y_pred_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this funciton will call the prior function\n",
    "def ensemble_glm_models(modeltypes, X_val=X_val, y_val=y_val, X_test_=X_test_, y_test_=y_test_, graph=True, splits=False):\n",
    "    all_models = []\n",
    "    all_mses = []\n",
    "    all_mses_train = []\n",
    "    all_rmses = []\n",
    "    all_rmses_train = []\n",
    "\n",
    "    if splits:\n",
    "        sets_list = []\n",
    "        for i in range(3, 20, 4):\n",
    "            X_train = X_val[X_val.index.isin(dates[:i])]\n",
    "            X_test = X_val[X_val.index.isin([dates[i]])]\n",
    "            y_train = y_val[y_val.index.isin(dates[:i])]\n",
    "            y_test = y_val[y_val.index.isin([dates[i]])]\n",
    "\n",
    "            sets_list.append([X_train, X_test, y_train, y_test])\n",
    "        return sets_list\n",
    "\n",
    "    # Lists to store base models for each modeltype\n",
    "    base_models = {modeltype: [] for modeltype in modeltypes}\n",
    "\n",
    "    for modeltype in modeltypes:\n",
    "        models = []\n",
    "        mses = []\n",
    "        mses_train = []\n",
    "        rmses = []\n",
    "        rmses_train = []\n",
    "\n",
    "        for i in range(3, 20, 4):\n",
    "            X_train = X_val[X_val.index.isin(dates[:i])]\n",
    "            X_test = X_val[X_val.index.isin([dates[i]])]\n",
    "            y_train = y_val[y_val.index.isin(dates[:i])]\n",
    "            y_test = y_val[y_val.index.isin([dates[i]])]\n",
    "\n",
    "            model, mse, mse_train, rmse, rmse_train, y_pred, y_pred_train = ensemble_models(X_train, X_test, y_train, y_test, ensemble_type=modeltype, graph=graph)\n",
    "\n",
    "            models.append(model)\n",
    "            mses.append(mse)\n",
    "            mses_train.append(mse_train)\n",
    "            rmses.append(rmse)\n",
    "            rmses_train.append(rmse_train)\n",
    "\n",
    "            print(f'Iteration {i + 1} - MSE: {mse[-1]:.4f} - RMSE: {rmse[-1]:.4f} - MSE train: {mse_train[-1]:.4f} - RMSE train: {rmse_train[-1]:.4f}')\n",
    "\n",
    "            # Fit the current base model\n",
    "            base_model = model\n",
    "            base_model.fit(X_train, y_train)\n",
    "\n",
    "            # Append the current base model to the list\n",
    "            base_models[modeltype].append((f'{modeltype}_model_{i}', base_model))\n",
    "\n",
    "        all_models.append(models)\n",
    "        all_mses.append(mses)\n",
    "        all_mses_train.append(mses_train)\n",
    "        all_rmses.append(rmses)\n",
    "        all_rmses_train.append(rmses_train)\n",
    "\n",
    "    # Create the final Voting Regressor outside the loop\n",
    "    ensemble_model = VotingRegressor(estimators=[])\n",
    "\n",
    "    # Aggregate all base models for each modeltype into the ensemble_model\n",
    "    for modeltype in modeltypes:\n",
    "        ensemble_model.estimators.extend(base_models[modeltype])\n",
    "\n",
    "    # Fit the final Voting Regressor with all base models\n",
    "    ensemble_model.fit(X_val, y_val)\n",
    "\n",
    "    # Retrieve predictions after fitting the ensemble model\n",
    "    y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "    y_pred_ensemble_train = ensemble_model.predict(X_val)\n",
    "\n",
    "    # Plot results for the Voting Regressor\n",
    "    if graph:\n",
    "        plt.scatter(X_test.index, y_test, label='Actual', color='blue', marker='o', s=20)\n",
    "        plt.scatter(X_test.index, y_pred_ensemble, label=f'Predicted (Ensemble)', marker='o', s=20, color='orange')\n",
    "        plt.title('Actual vs Predicted Values - Ensemble Model')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Total Medals')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    train_mse_ensemble = mean_squared_error(y_val, y_pred_ensemble_train)\n",
    "    train_rmse_ensemble = train_mse_ensemble**0.5\n",
    "    \n",
    "    mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
    "    rmse_ensemble = mse_ensemble**0.5\n",
    "    \n",
    "\n",
    "    print(f'Ensemble Model - Aggregate Metrics - MSE: {mse_ensemble:.4f} - RMSE: {rmse_ensemble:.4f}')\n",
    "    print(f'Ensemble Model - Aggregate Metrics - Train MSE: {train_mse_ensemble:.4f} - Train RMSE: {train_rmse_ensemble:.4f}')\n",
    "\n",
    "\n",
    "    return all_models, all_mses, all_mses_train, all_rmses, all_rmses_train, y_pred_ensemble, mse_ensemble, rmse_ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efffd4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modeltypes = ['xgboost', 'random_forest', 'gradient_boosting'] \n",
    "all_models, all_mses, all_mses_train, all_rmses, all_rmses_train, y_pred_ensemble, mse_ensemble, rmse_ensemble = ensemble_glm_models(modeltypes, X_val=X_val, y_val=y_val, graph=True, splits=False)\n",
    "print(f'cross validated mse: {np.mean(all_mses)}')\n",
    "print(f'cross validated mse train: {np.mean(all_mses_train)}')\n",
    "print(f'cross validated rmse: {np.mean(all_rmses)}')\n",
    "print(f'cross validated rmse train: {np.mean(all_rmses_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84235c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    results_dict.append({'model': f'{modeltypes[i]}',\n",
    "          'average_mse': np.mean(np.mean(all_mses[i])),\n",
    "          'average_rmse': np.mean(np.mean(all_rmses[i]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'model': 'Voting Regressor',\n",
    "          'average_mse': mse_ensemble,\n",
    "          'average_rmse': rmse_ensemble})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32422711",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858c59f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab42b8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2>Feature Importance for Random Forest</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7fb2b3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    As random forest performs remarkably well, we will use it to investigate the feature importance using the MDI. Our goal is to compare feature importance to the bootstrapped lasso regression, and finally come to a conclusion on which features truly play the largest role in predicting medals for the olympics. We will proceed by getting the original, unengineered data's importances, then follow with the interactions obtained from the lasso regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cac9e5",
   "metadata": {},
   "source": [
    "## Basic Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b74d46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This collects our 5 cross validation folds. Modeltype is not used because splits is True, \n",
    "# which means we only want the split cross validation data, nothing else\n",
    "\n",
    "\n",
    "\n",
    "# Function to get feature importances from the ensembles\n",
    "def ensemble_features(model, X_val=X_val):\n",
    "\n",
    "    feature_dict = {}\n",
    "    sets = glm_models(modeltype='LR',X_val=X_val,\n",
    "                  graph=False, splits=True)\n",
    "    \n",
    "\n",
    "    for i in range(5):\n",
    "        X_train, X_test, y_train, y_test = sets[i]\n",
    "        X_train.drop(columns='const',inplace=True)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        features = list(zip(X_train.columns, model.feature_importances_))\n",
    "\n",
    "        for feature_tuple in features:\n",
    "            key, value = feature_tuple\n",
    "            if key not in feature_dict.keys():\n",
    "                feature_dict[key] = [value]\n",
    "            else: \n",
    "                feature_dict[key].append(value)\n",
    "\n",
    "        return feature_dict\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6707ebb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_features_basic = ensemble_features(model=RandomForestRegressor(random_state=1),X_val=X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a53eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features_basic = {key: np.mean(value) for key, value in rf_features_basic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18738dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(rf_features_basic,h=12,title='Feature Importance for 5 Folds Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a3855",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "\n",
    "Based on this plot and findings so far, previous_year_medals, 3_lags_mean, event_count and contingent_size are always strong predictors. All predictors beyond that experience more variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da8352",
   "metadata": {},
   "source": [
    "## Feature Importance With Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_bootstrapped.remove('const')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0226c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_interactions = interactions[list(vars_bootstrapped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93af3b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_features = ensemble_features(model=RandomForestRegressor(random_state=1), \n",
    "                  name='Random Forest MDI Feature Importance', X_val=X_val_interactions, \n",
    "                  h=40, get_features=True, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ee6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dict = {key: np.mean(value) for key, value in rf_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(feats,h, title):\n",
    "    sorted_features = sorted(feats.items(), key=lambda x: x[1], reverse=False)\n",
    "    keys, values = zip(*sorted_features)\n",
    "\n",
    "    plt.figure(figsize=(12, h), dpi=100)\n",
    "    sns.set_style('white')\n",
    "    bars = plt.barh(keys, values, color='lightblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Feature Importance for {title}')\n",
    "    \n",
    "    for bar, value in zip(bars, values):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{value:.4f}', ha='left', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(rf_dict,h=20, title='Random Forest with Interactions, MDI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfce2eb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "\n",
    "It is interesting to see that winter and offers_incentive are not performing well. Since there are many variables with very low impurity decrease, we will use those with an impurity decrease equal to approximately 0.01 or more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = sorted(rf_dict.items(), key=lambda x: x[1], reverse=False)\n",
    "keys, values = zip(*sorted_features)\n",
    "keys = keys[-17:]\n",
    "values = values[-17:]\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "sns.set_style('white')\n",
    "bars = plt.barh(keys, values, color='lightblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title(f'Feature Importance for Features with Interactions in Random Forest')\n",
    "\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{value:.4f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de94bc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "\n",
    "The time features tend to to dominate, but features like gdp, contingent_size and even avg_temp come into play later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74076ad",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2> Tuning Ensemble Parameters </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6106fde",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    We will tune parameters in order to improve the performance of the voting regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(regressor,params):\n",
    "    '''tries all parameters from the specified parameter grid for the given model \n",
    "    and outputs the best ones for each of our 5 custom cross validation folds\n",
    "    \n",
    "    input: model, parameters\n",
    "    returns: best parameters in a dictionary for each cv fold and the corresponding negative MSE score\n",
    "    '''\n",
    "    splits = glm_models(modeltype='lR', graph=False, splits=True)\n",
    "    results=[]\n",
    "    params_chosen=[]\n",
    "    for i in range(5):\n",
    "        indxs=np.concatenate([-1*np.ones(len(X_train)), np.ones(len(X_test))])\n",
    "        pds = PredefinedSplit(test_fold = indxs)\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=regressor, param_grid=params, cv=pds, scoring='neg_mean_squared_error', \n",
    "                                   return_train_score=True)\n",
    "        grid_search.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))\n",
    "        params_chosen.append(grid_search.best_params_)\n",
    "        results.append(grid_search.best_score_)\n",
    "        \n",
    "    return grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ba7e0",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f60fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen by looking at scikit learn documentation\n",
    "# Unfortunately we forgot to add subsample to test with bootstrapping. \n",
    "# We will try it when fitting the actual model as the grid search is too time consuming.\n",
    "\n",
    "params=[{'bootstrap': [True, False],\n",
    "        'criterion': ['poisson', 'friedman_mse', 'squared_error'],\n",
    "       'n_estimators': [10,20,30,50,100,200,500],\n",
    "       'max_depth': [None] + [10,20,50,70,100],\n",
    "        'max_features':['sqrt', 'log2'],\n",
    "        'ccp_alpha': [0,0.001, 0.01, 0.1]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4076bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751823b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune(rf,params=params)\n",
    "# The code above is what we used to generate these parameters\n",
    "# We had to copy and paste the results as we forgot to assign the search to a variable\n",
    "\n",
    "rf_params = [{'bootstrap': True, 'ccp_alpha': 0.1, 'criterion': 'squared_error', 'max_depth': 70, 'max_features': 'sqrt', 'n_estimators': 30}, \n",
    "{'bootstrap': False, 'ccp_alpha': 0.1, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 10},\n",
    "{'bootstrap': True, 'ccp_alpha': 0.001, 'criterion': 'poisson', 'max_depth': 50, 'max_features': 'sqrt', 'n_estimators': 10},\n",
    "{'bootstrap': True, 'ccp_alpha': 0.1, 'criterion': 'friedman_mse', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 20},\n",
    "{'bootstrap': True, 'ccp_alpha': 0.1, 'criterion': 'friedman_mse', 'max_depth': 100, 'max_features': 'log2', 'n_estimators': 10}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params_df = pd.DataFrame(rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17694238",
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = [11.853142375397121,11.942884481051081,11.310826408766427,12.029059264391247,12.056208953844926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93541f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = [mse**0.5 for mse in mses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537932ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params_df['mse']=mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adbebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params_df['rmse']=rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd20b89",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "\n",
    "MSE decreased a lot compared to the baseline linear regression model, only predicting about 3 medals off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a9d45",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525306cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr=GradientBoostingRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7026b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[{'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "         'eta': [0.001, 0.01, 0.1],\n",
    "         'min_split_loss': [0.1, 0.2, 0.5],\n",
    "         'subsample':[0.8, 1.0]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_xgb = tune(xgbr,params=params)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbda9253",
   "metadata": {},
   "source": [
    "{'mean_fit_time': array([0.68527603, 0.86821508, 0.69896388, 0.75335789, 0.68544316,\n",
    "        0.56169009, 0.56925321, 0.84463596, 0.52928686, 0.57648683,\n",
    "        0.5121119 , 0.62161708, 0.73991203, 0.77062726, 0.78978515,\n",
    "        0.74980903, 0.63969994, 0.86579871, 0.07277393, 0.04014397,\n",
    "        0.0388658 , 0.04449105, 0.05599999, 0.04073501, 0.07219672,\n",
    "        0.07214904, 0.08526802, 0.04049802, 0.08335209, 0.06965995,\n",
    "        0.07270813, 0.04463005, 0.08745313, 0.04085112, 0.04130411,\n",
    "        0.05703402, 1.44820809, 1.49922109, 1.46064782, 1.51017976,\n",
    "        1.44279408, 1.85272503, 1.60918689, 1.84443712, 1.88721418,\n",
    "        2.04418373, 1.85169196, 1.77830911, 1.62164807, 1.91847396,\n",
    "        1.865062  , 1.67506981, 3.00255489, 2.36574006]),\n",
    " 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0.]),\n",
    " 'mean_score_time': array([0.00321603, 0.00297379, 0.00275707, 0.00306106, 0.00269079,\n",
    "        0.00267887, 0.00262713, 0.00261807, 0.00278902, 0.00264812,\n",
    "        0.00261307, 0.00276399, 0.00355196, 0.00266171, 0.00319099,\n",
    "        0.00263691, 0.00273609, 0.00307631, 0.002105  , 0.00195193,\n",
    "        0.00196195, 0.00236511, 0.0020771 , 0.00219893, 0.00280213,\n",
    "        0.00229502, 0.00248599, 0.00196075, 0.00260377, 0.00218606,\n",
    "        0.00343394, 0.00218201, 0.00196004, 0.00197315, 0.00201082,\n",
    "        0.00195193, 0.03563905, 0.03569603, 0.03661704, 0.03560495,\n",
    "        0.03563523, 0.039217  , 0.04025412, 0.03577304, 0.03553104,\n",
    "        0.03617001, 0.04027224, 0.03626299, 0.04256606, 0.04139614,\n",
    "        0.03639603, 0.04138184, 0.15203094, 0.04801393]),\n",
    " 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0.]),\n",
    " 'param_booster': masked_array(data=['gbtree', 'gbtree', 'gbtree', 'gbtree', 'gbtree',\n",
    "                    'gbtree', 'gbtree', 'gbtree', 'gbtree', 'gbtree',\n",
    "                    'gbtree', 'gbtree', 'gbtree', 'gbtree', 'gbtree',\n",
    "                    'gbtree', 'gbtree', 'gbtree', 'gblinear', 'gblinear',\n",
    "                    'gblinear', 'gblinear', 'gblinear', 'gblinear',\n",
    "                    'gblinear', 'gblinear', 'gblinear', 'gblinear',\n",
    "                    'gblinear', 'gblinear', 'gblinear', 'gblinear',\n",
    "                    'gblinear', 'gblinear', 'gblinear', 'gblinear', 'dart',\n",
    "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
    "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
    "                    'dart', 'dart', 'dart'],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'param_eta': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
    "                    0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
    "                    0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
    "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
    "                    0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'param_min_split_loss': masked_array(data=[0.1, 0.1, 0.2, 0.2, 0.5, 0.5, 0.1, 0.1, 0.2, 0.2, 0.5,\n",
    "                    0.5, 0.1, 0.1, 0.2, 0.2, 0.5, 0.5, 0.1, 0.1, 0.2, 0.2,\n",
    "                    0.5, 0.5, 0.1, 0.1, 0.2, 0.2, 0.5, 0.5, 0.1, 0.1, 0.2,\n",
    "                    0.2, 0.5, 0.5, 0.1, 0.1, 0.2, 0.2, 0.5, 0.5, 0.1, 0.1,\n",
    "                    0.2, 0.2, 0.5, 0.5, 0.1, 0.1, 0.2, 0.2, 0.5, 0.5],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'param_subsample': masked_array(data=[0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8,\n",
    "                    1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0,\n",
    "                    0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8,\n",
    "                    1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0,\n",
    "                    0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 1.0],\n",
    "              mask=[False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False, False, False,\n",
    "                    False, False, False, False, False, False],\n",
    "        fill_value='?',\n",
    "             dtype=object),\n",
    " 'params': [{'booster': 'gbtree',\n",
    "   'eta': 0.001,\n",
    "   'min_split_loss': 0.1,\n",
    "   'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.001, 'min_split_loss': 0.1, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.001, 'min_split_loss': 0.2, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.001, 'min_split_loss': 0.2, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.001, 'min_split_loss': 0.5, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.001, 'min_split_loss': 0.5, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.01, 'min_split_loss': 0.1, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.01, 'min_split_loss': 0.1, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.01, 'min_split_loss': 0.2, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.01, 'min_split_loss': 0.2, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.01, 'min_split_loss': 0.5, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.01, 'min_split_loss': 0.5, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.1, 'min_split_loss': 0.1, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.1, 'min_split_loss': 0.1, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.1, 'min_split_loss': 0.2, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.1, 'min_split_loss': 0.2, 'subsample': 1.0},\n",
    "  {'booster': 'gbtree', 'eta': 0.1, 'min_split_loss': 0.5, 'subsample': 0.8},\n",
    "  {'booster': 'gbtree', 'eta': 0.1, 'min_split_loss': 0.5, 'subsample': 1.0},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.001,\n",
    "   'min_split_loss': 0.1,\n",
    "   'subsample': 0.8},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.001,\n",
    "   'min_split_loss': 0.1,\n",
    "   'subsample': 1.0},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.001,\n",
    "   'min_split_loss': 0.2,\n",
    "   'subsample': 0.8},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.001,\n",
    "   'min_split_loss': 0.2,\n",
    "   'subsample': 1.0},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.001,\n",
    "   'min_split_loss': 0.5,\n",
    "   'subsample': 0.8},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.001,\n",
    "   'min_split_loss': 0.5,\n",
    "   'subsample': 1.0},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.01,\n",
    "   'min_split_loss': 0.1,\n",
    "   'subsample': 0.8},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.01,\n",
    "   'min_split_loss': 0.1,\n",
    "   'subsample': 1.0},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.01,\n",
    "   'min_split_loss': 0.2,\n",
    "   'subsample': 0.8},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.01,\n",
    "   'min_split_loss': 0.2,\n",
    "   'subsample': 1.0},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.01,\n",
    "   'min_split_loss': 0.5,\n",
    "   'subsample': 0.8},\n",
    "  {'booster': 'gblinear',\n",
    "   'eta': 0.01,\n",
    "   'min_split_loss': 0.5,\n",
    "   'subsample': 1.0},\n",
    "  {'booster': 'gblinear', 'eta': 0.1, 'min_split_loss': 0.1, 'subsample': 0.8},\n",
    "  {'booster': 'gblinear', 'eta': 0.1, 'min_split_loss': 0.1, 'subsample': 1.0},\n",
    "  {'booster': 'gblinear', 'eta': 0.1, 'min_split_loss': 0.2, 'subsample': 0.8},\n",
    "  {'booster': 'gblinear', 'eta': 0.1, 'min_split_loss': 0.2, 'subsample': 1.0},\n",
    "  {'booster': 'gblinear', 'eta': 0.1, 'min_split_loss': 0.5, 'subsample': 0.8},\n",
    "  {'booster': 'gblinear', 'eta': 0.1, 'min_split_loss': 0.5, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.001, 'min_split_loss': 0.1, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.001, 'min_split_loss': 0.1, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.001, 'min_split_loss': 0.2, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.001, 'min_split_loss': 0.2, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.001, 'min_split_loss': 0.5, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.001, 'min_split_loss': 0.5, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.01, 'min_split_loss': 0.1, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.01, 'min_split_loss': 0.1, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.01, 'min_split_loss': 0.2, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.01, 'min_split_loss': 0.2, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.01, 'min_split_loss': 0.5, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.01, 'min_split_loss': 0.5, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.1, 'min_split_loss': 0.1, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.1, 'min_split_loss': 0.1, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.1, 'min_split_loss': 0.2, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.1, 'min_split_loss': 0.2, 'subsample': 1.0},\n",
    "  {'booster': 'dart', 'eta': 0.1, 'min_split_loss': 0.5, 'subsample': 0.8},\n",
    "  {'booster': 'dart', 'eta': 0.1, 'min_split_loss': 0.5, 'subsample': 1.0}],\n",
    " 'split0_test_score': array([-408.02178492, -405.36929987, -408.02178492, -405.36929973,\n",
    "        -408.02178492, -405.36930211, -103.52614186, -107.58669322,\n",
    "        -103.5261469 , -107.58698767, -103.4778171 , -107.58693262,\n",
    "         -20.14071014,  -21.25289574,  -19.7320295 ,  -21.70503171,\n",
    "         -20.80793609,  -22.02578252,  -69.76226024,  -69.76741058,\n",
    "         -69.76728761,  -69.7681119 ,  -69.77773878,  -69.76489632,\n",
    "         -24.43245068,  -24.35842676,  -24.37876001,  -24.35657692,\n",
    "         -24.40091455,  -24.4225993 ,  -19.50490369,  -19.37667086,\n",
    "         -19.51640988,  -19.523118  ,  -19.24859725,  -19.55131847,\n",
    "        -407.95507148, -405.36930159, -407.95227993, -405.36930159,\n",
    "        -407.95271806, -405.369304  , -103.61332466, -107.58669963,\n",
    "        -103.61383095, -107.58699467, -103.73533273, -107.5869333 ,\n",
    "         -19.82642156,  -21.25289699,  -20.44287917,  -21.70503668,\n",
    "         -20.07680908,  -22.02578345]),\n",
    " 'mean_test_score': array([-408.02178492, -405.36929987, -408.02178492, -405.36929973,\n",
    "        -408.02178492, -405.36930211, -103.52614186, -107.58669322,\n",
    "        -103.5261469 , -107.58698767, -103.4778171 , -107.58693262,\n",
    "         -20.14071014,  -21.25289574,  -19.7320295 ,  -21.70503171,\n",
    "         -20.80793609,  -22.02578252,  -69.76226024,  -69.76741058,\n",
    "         -69.76728761,  -69.7681119 ,  -69.77773878,  -69.76489632,\n",
    "         -24.43245068,  -24.35842676,  -24.37876001,  -24.35657692,\n",
    "         -24.40091455,  -24.4225993 ,  -19.50490369,  -19.37667086,\n",
    "         -19.51640988,  -19.523118  ,  -19.24859725,  -19.55131847,\n",
    "        -407.95507148, -405.36930159, -407.95227993, -405.36930159,\n",
    "        -407.95271806, -405.369304  , -103.61332466, -107.58669963,\n",
    "        -103.61383095, -107.58699467, -103.73533273, -107.5869333 ,\n",
    "         -19.82642156,  -21.25289699,  -20.44287917,  -21.70503668,\n",
    "         -20.07680908,  -22.02578345]),\n",
    " 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0.]),\n",
    " 'rank_test_score': array([52, 44, 52, 43, 52, 47, 32, 37, 33, 41, 31, 39, 10, 13,  7, 15, 12,\n",
    "        17, 25, 28, 27, 29, 30, 26, 24, 20, 21, 19, 22, 23,  3,  2,  4,  5,\n",
    "         1,  6, 51, 45, 49, 45, 50, 48, 34, 38, 35, 42, 36, 40,  8, 14, 11,\n",
    "        16,  9, 18], dtype=int32),\n",
    " 'split0_train_score': array([-327.9875312 , -327.15652271, -327.9875312 , -327.15652258,\n",
    "        -327.9875312 , -327.15655115,  -77.60552897,  -75.58661296,\n",
    "         -77.60553912,  -75.58647575,  -77.61953469,  -75.58647758,\n",
    "          -1.07058567,   -1.04042423,   -1.10445651,   -1.03284125,\n",
    "          -1.06014744,   -1.1358539 ,  -70.22409601,  -70.22064313,\n",
    "         -70.22086836,  -70.22116361,  -70.22705491,  -70.21856204,\n",
    "         -36.84327702,  -36.85223461,  -36.82090191,  -36.85114856,\n",
    "         -36.83648023,  -36.84048044,  -28.8357013 ,  -28.75761119,\n",
    "         -28.73144609,  -28.85858758,  -28.72474004,  -28.85607361,\n",
    "        -328.15702314, -327.15652627, -328.15675622, -327.15652641,\n",
    "        -328.15735158, -327.15655557,  -78.10121321,  -75.58661416,\n",
    "         -78.09983738,  -75.58647751,  -78.06910482,  -75.58647678,\n",
    "          -1.07939823,   -1.04042416,   -1.17897561,   -1.03284116,\n",
    "          -1.23095425,   -1.13585388]),\n",
    " 'mean_train_score': array([-327.9875312 , -327.15652271, -327.9875312 , -327.15652258,\n",
    "        -327.9875312 , -327.15655115,  -77.60552897,  -75.58661296,\n",
    "         -77.60553912,  -75.58647575,  -77.61953469,  -75.58647758,\n",
    "          -1.07058567,   -1.04042423,   -1.10445651,   -1.03284125,\n",
    "          -1.06014744,   -1.1358539 ,  -70.22409601,  -70.22064313,\n",
    "         -70.22086836,  -70.22116361,  -70.22705491,  -70.21856204,\n",
    "         -36.84327702,  -36.85223461,  -36.82090191,  -36.85114856,\n",
    "         -36.83648023,  -36.84048044,  -28.8357013 ,  -28.75761119,\n",
    "         -28.73144609,  -28.85858758,  -28.72474004,  -28.85607361,\n",
    "        -328.15702314, -327.15652627, -328.15675622, -327.15652641,\n",
    "        -328.15735158, -327.15655557,  -78.10121321,  -75.58661416,\n",
    "         -78.09983738,  -75.58647751,  -78.06910482,  -75.58647678,\n",
    "          -1.07939823,   -1.04042416,   -1.17897561,   -1.03284116,\n",
    "          -1.23095425,   -1.13585388]),\n",
    " 'std_train_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0.])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f78a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "-18**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ba6a8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2> Choosing a Model and Predicting </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56c7fc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2> Limitations and Future Work </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9319724",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E6F7FF; border-radius: 10px; border: 2px solid #007bff; padding: 10px;\">\n",
    "    One of the main limitations of this work is that it does not address decimal predictions. Since the data is in counts, yet most models do not accomodate it, we did not predict in full medal values. One option would be to round down, since most models were likelier to overpredict. However, this should be researched accordingly. \n",
    "    <br><br>Another limitation is that cross validation is not identical in time series data. While we created appropriate validation folds, comparing the train data to the test is problematic. It is not common to check model performance on the training data in time series, as we are predicting into the future: T-n is used to predict T, not the other way around. Assessing issues like overfitting therefore became more complicated and was not addressed sufficiently in this work. <br><br>The overprediction issue of the GLMs was never sufficiently solved. Though this project attempted to scale and transform accordingly, the results were not as expected. \n",
    "    <br><br> Throughout this project, the feature importance was obtained through various methods. When many features were involved, coefficients were close to zero for several, though they were not turned off completely (due to the bootstrapping). In order to gain more understanding for which features are acceptable to remove, forward feature selection would have been useful. \n",
    "    <br><br> One of the main shortcomings of this project is the way in which MSEs are compared. Though we take the average of all cross validation folds in order to stay robust, it was apparent that the first two folds performed poorly. Though this is part of cross validation, in time series, it may make sense to limit the validation window if it is better suited to the problem at hand. Based on what we observed, one year into the past or even 8 is not sufficient. Since there is enough data, we recommend this prediction problem is readdressed using a different scoring methodology, or cross validation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de75c47",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3dcca0",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #007bff; padding: 15px; border-radius: 10px; color: #ffffff;\">\n",
    "    <h2> Will a participating athlete win a medal or not? </h2>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a305a8e",
   "metadata": {},
   "source": [
    "Let's merge the athlete data with the event results data to build a dataframe for our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf385d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#merge athlete with event results\n",
    "df_athletes_events = pd.merge(df_athletes[['name', 'sex', 'born', 'height', 'weight', 'country_noc']],\n",
    "                              df_event_results[['athlete','edition', 'sport', 'event', 'medal', 'isTeamSport']],\n",
    "                              left_on='name', right_on='athlete', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with games\n",
    "\n",
    "df_athletes_events2 = pd.merge(df_athletes_events, df_games[['edition', 'year', 'country_noc']],on='edition', how='left')\n",
    "\n",
    "df_athletes_events2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get age during the games\n",
    "\n",
    "df_athletes_events2['born'] = pd.to_datetime(df_athletes_events2['born'], errors='coerce')\n",
    "df_athletes_events2['age'] = df_athletes_events2['year'] - df_athletes_events2['born'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a column for edition\n",
    "df_athletes_events2['edition_type'] = df_athletes_events2['edition'].apply(lambda x: 'Summer' if 'Summer' in x else ('Winter' if 'Winter' in x else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77777eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athletes_events2.rename({'country_noc_y': 'host_country', 'country_noc_x':'country_noc'}, axis=1, inplace=True)\n",
    "df_athletes_events2.drop(['athlete', 'edition'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we observed earlier that height and wight doesn't have a significant impact on choice of sport, so let us impute the mean\n",
    "df_athletes_events2['height'] = pd.to_numeric(df_athletes_events2['height'], errors='coerce')  \n",
    "mean_h= df_athletes_events2['height'].mean()\n",
    "df_athletes_events2['height'].fillna(mean_h, inplace=True)\n",
    "\n",
    "df_athletes_events2['weight'] = pd.to_numeric(df_athletes_events2['weight'], errors='coerce')  \n",
    "mean_w= df_athletes_events2['weight'].mean()\n",
    "df_athletes_events2['weight'].fillna(mean_h, inplace=True)\n",
    "\n",
    "df_athletes_events2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b782aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athletes_events2.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a10e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athletes_events2['isTeamSport'] = df_athletes_events2['isTeamSport'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a75d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column stating whether the athlete is from the host country or not\n",
    "df_athletes_events2['from_host_country'] = np.where(df_athletes_events2['country_noc'] == \n",
    "                                                    df_athletes_events2['host_country'], 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0cec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this model,we are going to focus only on the last summer and winter olympics, i.e. 2020 and 2022\n",
    "\n",
    "#number of olympics an athlete participated in \n",
    "olympic_count = df_athletes_events2.groupby(['name', 'country_noc'])['year'].nunique()\n",
    "olympic_count = pd.DataFrame(olympic_count).reset_index()\n",
    "olympic_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of events participating in at the 2020 and 2022 Olympics \n",
    "\n",
    "df_2020_22 = df_athletes_events2.loc[df_athletes_events2['year'].isin([2020, 2022])]\n",
    "df_2020_22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecdb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cnt = df_2020_22.groupby(['name', 'country_noc'])['event'].nunique()\n",
    "event_cnt = pd.DataFrame(event_cnt).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2020_22 will be the df for the model, need to add the extra cols created\n",
    "\n",
    "\n",
    "# Group by 'athlete' and check if an athlete has both values for 'isTeamSport'\n",
    "df_2020_22['participation'] = df_2020_22.groupby('name')['isTeamSport'].transform(lambda x: 'Both' if x.nunique() > 1 else x.iloc[0])\n",
    "df_2020_22['participation'] = df_2020_22['participation'].replace({0: 'Individual', 1: 'Team'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Will an athlete win a medal or not?\n",
    "df_2020_22['won_medal'] = (df_2020_22['medal'] != 'na').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets group by name and medal_won since there are multiple events an athlete might participate in \n",
    "\n",
    "medal_cnt = df_2020_22.groupby(['name', 'country_noc'])['won_medal'].max()\n",
    "medal_cnt = pd.DataFrame(medal_cnt).reset_index()\n",
    "medal_cnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#did they win medals in the past\n",
    "\n",
    "df_not_2020_22 = df_athletes_events2.loc[~df_athletes_events2['year'].isin([2020, 2022])]\n",
    "df_not_2020_22['won_medal'] = (df_not_2020_22['medal'] != 'na').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_medals = df_not_2020_22.groupby(['name', 'country_noc'])['won_medal'].sum()\n",
    "past_medals = pd.DataFrame(past_medals).reset_index()\n",
    "past_medals.drop_duplicates(inplace=True)\n",
    "past_medals.rename({'won_medal':'prior_medals'}, axis=1, inplace=True)\n",
    "past_medals.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start constructing final dataframe now\n",
    "\n",
    "classification_df = df_2020_22.drop(['event','born', 'medal', 'won_medal', 'year','isTeamSport'], axis=1)\n",
    "classification_df.drop_duplicates(inplace=True)\n",
    "classification_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute mean age for missing values\n",
    "\n",
    "classification_df['age'] = pd.to_numeric(classification_df['age'], errors='coerce')  \n",
    "mean_age= classification_df['age'].mean()\n",
    "classification_df['age'].fillna(mean_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f04fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge other dfs\n",
    "\n",
    "classification_df2 = pd.merge(classification_df, medal_cnt[['name', 'country_noc', 'won_medal']],\n",
    "                              on=['name', 'country_noc'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded62a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df3 = pd.merge(classification_df2, event_cnt[['name', 'country_noc', 'event']],\n",
    "                              on=['name', 'country_noc'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df4 = pd.merge(classification_df3, olympic_count[['name', 'country_noc', 'year']],\n",
    "                              on=['name', 'country_noc'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5276bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df4.rename({'event': 'event_cnt', 'year': 'olympic_cnt'}, axis=1, inplace=True)\n",
    "classification_df4.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df5 = pd.merge(classification_df4, past_medals[['name', 'country_noc', 'prior_medals']],\n",
    "                              on=['name', 'country_noc'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution - imbalanced \n",
    "\n",
    "classification_df5['won_medal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df5.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eda172",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df5.drop(['name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948e8ab",
   "metadata": {},
   "source": [
    "In this model, we will specifically target athletes hailing from the top 50 countries boasting the highest historical medal counts, thereby addressing potential dimensionality concerns. The fact that the lower 75% of the dataset has collectively secured only 98 medals throughout all historical games highlights the dominance of a select few in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7958c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(data=df_medals_agg, x='total', bins=30, kde=True)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Total Medals')\n",
    "plt.ylabel('Number of Countries')\n",
    "plt.title('Distribution of Total Medals by Country')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df6 = classification_df5.loc[classification_df5['country_noc'].isin(top_50.country_noc)]\n",
    "classification_df6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc989b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate model results \n",
    "\n",
    "def evaluate_model(true_labels, predicted_labels, train_true_labels=None, train_predicted_labels=None, model_name=None, model_number=None, results_df=None):\n",
    "    # Calculate F1 score for test data\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Calculate Precision, Recall, and Accuracy for test data\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Calculate AUC for test data\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predicted_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Create a confusion matrix for test data\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix (Test Data)')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the ROC curve for test data\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve (Test Data)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate F1 score for train data\n",
    "    train_f1 = f1_score(train_true_labels, train_predicted_labels)\n",
    "    \n",
    "    # Save metrics in a DataFrame\n",
    "    model_results_df = pd.DataFrame({ \n",
    "        'Model No.': [model_number],\n",
    "        'Model': [model_name],\n",
    "        'F1 Score (Test Data)': [f1],\n",
    "        'F1 Score (Train Data)': [train_f1],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'Accuracy': [accuracy],\n",
    "        'AUC': [roc_auc]\n",
    "    })\n",
    "    \n",
    "    # Append to the existing results DataFrame\n",
    "    if results_df is None:\n",
    "        results_df = model_results_df\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, model_results_df], ignore_index=True)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, X_train, top_n=30):\n",
    "    #  extract feature names from preprocessir\n",
    "    if 'preprocessor' in model.named_steps:\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        numeric_features = preprocessor.transformers_[0][2]\n",
    "        categorical_features = preprocessor.transformers_[1][1]\\\n",
    "            .named_steps['onehot'] \\\n",
    "            .get_feature_names_out(preprocessor.transformers_[1][2])\n",
    "        feature_names = np.concatenate([numeric_features, categorical_features])\n",
    "    else:\n",
    "        # If no preprocessor, assume feature names are directly from X_train\n",
    "        feature_names = X_train.columns\n",
    "\n",
    "    # Access the feature importances from the model\n",
    "    feature_importances = model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Create a dictionary with feature names and their importance scores\n",
    "    feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "    sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select the top features\n",
    "    top_features = dict(sorted_feature_importance[:top_n])\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=list(top_features.values()), y=list(top_features.keys()), palette='viridis')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Top Feature Importance')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba013ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f40d0f",
   "metadata": {},
   "source": [
    "We will leverage the scikit-learn pipeline for our modeling process.\n",
    "\n",
    "<br>\n",
    "We will begin by building a baseline random forest model. The features going into the model include:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Feature</th>\n",
    "    <th>Type</th>\n",
    "    <th>Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>height</td>\n",
    "    <td>Numeric</td>\n",
    "    <td>Height of the athlete</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>weight</td>\n",
    "    <td>Numeric</td>\n",
    "    <td>Weight of the athlete</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>age</td>\n",
    "    <td>Numeric</td>\n",
    "    <td>Age of the athlete</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>event_cnt</td>\n",
    "    <td>Numeric</td>\n",
    "    <td>No. of events the athlete is participating in at the Games</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>olympic_cnt</td>\n",
    "    <td>Numeric</td>\n",
    "    <td>No. of prior Olympics the athlete has participated in</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>prior_medals</td>\n",
    "    <td>Numeric</td>\n",
    "    <td>No. of medals won in prior games</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sex</td>\n",
    "    <td>Categorical</td>\n",
    "    <td>Male or Female</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>country_noc</td>\n",
    "    <td>Categorical</td>\n",
    "    <td>Country the athlete is representing</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sport</td>\n",
    "    <td>Categorical</td>\n",
    "    <td>Sport the athlete plays</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>participation</td>\n",
    "    <td>Categorical</td>\n",
    "    <td>Participating in individual, team, or both events</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>edition_type</td>\n",
    "    <td>Categorical</td>\n",
    "    <td>Summer or Winter Olympics</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>from_host_country</td>\n",
    "    <td>Categorical</td>\n",
    "    <td>Whether the athlete is from the host country or not</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5099975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model - Random forest\n",
    "\n",
    "X = classification_df6.drop('won_medal', axis=1)\n",
    "y = classification_df6['won_medal']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features = ['height', 'weight', 'age', 'event_cnt', 'olympic_cnt', 'prior_medals']\n",
    "categorical_features = ['sex', 'country_noc', 'sport', 'participation', 'edition_type', 'from_host_country']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Fit and predict using the model\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fe3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model(y_test, y_pred, y_train, y_pred_train, 'Random Forest', '1', results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6009c",
   "metadata": {},
   "source": [
    "This model is over-fitting, it could be due to noisy data, too many features and also the fact that the data is imabalanced. We shall now try over and under sampling to see if that makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6efde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1.b with oversampling: try random forest with resampling \n",
    "\n",
    "# Oversample the minority class using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "model1b = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Fit the model on the resampled data\n",
    "model1b.fit(X_resampled, y_resampled)\n",
    "\n",
    "\n",
    "y_pred1b = model1b.predict(X_test)\n",
    "y_pred1b_train = model1b.predict(X_train)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred1b))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred1b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model(y_test, y_pred1b, y_train, y_pred1b_train, 'Random Forest with oversampling', '1b', results_df)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc5ccc",
   "metadata": {},
   "source": [
    "Oversampling still overfits as measured by the F1 score for the train data. This may be due to the fact that Random Forests, being an ensemble of decision trees, end up memorizing this duplicated information, leading to overfitting. Oversampling can also affect the decision boundaries of individual trees in the Random Forest. If decision boundaries become too tailored to the oversampled instances, the ensemble may struggle to generalize well to new and unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1c: Try undersampling\n",
    "\n",
    "\n",
    "# Undersample the majority class using RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_underresampled, y_underresampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "\n",
    "model.fit(X_underresampled, y_underresampled)\n",
    "\n",
    "\n",
    "y_pred1c = model.predict(X_test)\n",
    "y_pred1c_train = model.predict(X_train)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred1c))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred1c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239db8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model(y_test, y_pred1c, y_train, y_pred1c_train, 'Random Forest with undersampling', '1c', results_df)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c3de2",
   "metadata": {},
   "source": [
    "Undersampling did not lead to overfitting. Since it involves reducing the number of instances from the majority class, it can simplify the training set and prevent the model from memorizing specific instances and overfitting. This model also has the best AUC of the three but the F1 score on the test set is the worst.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb8e73",
   "metadata": {},
   "source": [
    "Let us move on to boosting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2.1 :XGBoost\n",
    "\n",
    "model2 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', xgb.XGBClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__scale_pos_weight': [None, 4]  # Adjusted scale_pos_weight values to account for imbalance \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model2, param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "y_pred2 = grid_search.best_estimator_.predict(X_test)\n",
    "y_pred2_train = grid_search.best_estimator_.predict(X_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred2))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(grid_search.best_estimator_, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model(y_test, y_pred2, y_train, y_pred2_train, 'XGBoost', '2', results_df)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436ee2e",
   "metadata": {},
   "source": [
    "XGBoost performed the best so far. Since it builds trees sequentially, with each tree correcting the errors of the previous ones, the process often leads to better overall performance compared to the independent learning of trees in Random Forest. Adjusting the scale_pos_weight to account for the imbalance in classes improved model performance significantly. The AUC has also gone up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4aaae",
   "metadata": {},
   "source": [
    "Let's try xgboost with oversampling and see what that looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1cb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model2.2 :XGBoost with oversampling\n",
    "\n",
    "model2 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', xgb.XGBClassifier(random_state=42))])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__n_estimators': [50, 100, 200]\n",
    "    \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model2, param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Obtain the best estimator from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred2b = best_model.predict(X_test)\n",
    "y_pred2b_train = best_model.predict(X_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred2b))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred2b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model(y_test, y_pred2b, y_train, y_pred2b_train, 'XGBoost with oversampling', '2b', results_df)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8380d8",
   "metadata": {},
   "source": [
    "Adjusting the scale_pos_weight which is part of the algorithm is more efficient than oversampling which is why there is a slight difference in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87920b8",
   "metadata": {},
   "source": [
    "Let's try bagging now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba74fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model3: Bagging with RFC as base_estimator\n",
    "\n",
    "bagging_model = BaggingClassifier(base_estimator=RandomForestClassifier(random_state=42),\n",
    "                                  n_estimators=5) \n",
    "\n",
    "bagging_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('classifier', bagging_model)])\n",
    "\n",
    "bagging_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = bagging_pipeline.predict(X_test)\n",
    "\n",
    "y_pred3_train = bagging_pipeline.predict(X_train)\n",
    "\n",
    "                            \n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred3))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc14f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model(y_test, y_pred3, y_train, y_pred3_train, 'Bagging', '3', results_df)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b5dc0",
   "metadata": {},
   "source": [
    "The bagging model also seems like it was overfitting, but not as much as the Random Forest models. This could be because there is a significant amount of noise in the training data and bagging might inadvertently amplify this noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911789e",
   "metadata": {},
   "source": [
    "We will also try a Logistic Regression model, athough it is likely to underperform the other models by a mile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da85db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 4 Log reg\n",
    "\n",
    "log_reg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "log_reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('classifier',log_reg_model)])\n",
    "\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred4 = log_reg_pipeline.predict(X_test)\n",
    "y_pred4_train = log_reg_pipeline.predict(X_train)\n",
    "\n",
    "                            \n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred4))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff813345",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model(y_test, y_pred4, y_train, y_pred4_train, 'Logistic Regression', '4', results_df)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f562ef",
   "metadata": {},
   "source": [
    "As expected, Logistic Regression performed poorly compared to the other models. The primary reason for this could be imbalanced data. Also, if the true relationship is highly non-linear, decision tree-based models like random forest or boosting algorithms can capture these non-linearities better and they can better represent the decision boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68caf047",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8dd87b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFFE0; border: 2px solid #FFD700; padding: 20px;\">\n",
    "\n",
    "\n",
    "Based on the results from the different models, several observations and conclusions can be drawn:\n",
    "\n",
    "<strong>Random Forest Models: </strong>\n",
    "\n",
    "The Random Forest model without resampling (Model 1) achieved a high F1 score on the training data (0.98683) but showed a drop in performance on the test data (F1 score: 0.70362), indicating potential overfitting.\n",
    "Random Forest with oversampling (Model 1b) improved the F1 score on the test data (0.71901), suggesting that oversampling had a positive impact on generalization but there is still overfitting.\n",
    "Random Forest with undersampling (Model 1c) showed a lower F1 score on the test data (0.65544), indicating that undersampling might not be as effective in this context.\n",
    "<br> <br>\n",
    "<strong>\n",
    "XGBoost Models:</strong>\n",
    "   <br> <br>\n",
    "The XGBoost model without resampling (Model 2) achieved a competitive F1 score on the test data (0.71339), and it performed well on both training and test datasets.\n",
    "XGBoost with oversampling (Model 2b) demonstrated consistent performance on the test data (F1 score: 0.70696) while maintaining a better balance between precision and recall.\n",
    "<br> <br>\n",
    "<strong>\n",
    "Bagging Model:\n",
    "</strong>\n",
    "    <br> <br>\n",
    "The Bagging model (Model 3) achieved a relatively high F1 score on the training data (0.91069) but had lower performance on the test data (F1 score: 0.66587), indicating potential overfitting.\n",
    "<br> <br>\n",
    "<strong>\n",
    "Logistic Regression Model:\n",
    "</strong>\n",
    "    <br> <br>\n",
    "The Logistic Regression model (Model 4) had the lowest F1 score on the test data (0.26180), indicating challenges in correctly identifying positive instances. The precision and recall values are also relatively low.\n",
    "<br> <br>\n",
    "<strong>\n",
    "Recommendation:\n",
    "</strong>\n",
    "    <br> <br>\n",
    "XGBoost without resampling (Model 2) and XGBoost with oversampling (Model 2b) are promising models, demonstrating good generalization performance.\n",
    "Random Forest with oversampling (Model 1b) also shows improvement over the non-resampled version.\n",
    "Further exploration of hyperparameter tuning for XGBoost and Random Forest models may lead to improved performance.\n",
    "<br><br>\n",
    "    <strong>Future work: </strong><br><br>\n",
    "Some of the ways we can further enhance these models include - \n",
    "\n",
    "1. Hyperparameter tuning - spend more time tuning the models and explore the effect of regularization\n",
    "\n",
    "2. Additional data - we can explore additional data points like results at other events, age the athlete started training, etc. and assess their impact on model performance \n",
    "\n",
    "3. Explore interaction terms - interactions between certain features might have a combined effect on the target\n",
    "4. Investigate additional features or alternative data preprocessing techniques to enhance the model's ability to capture underlying patterns.\n",
    "5. Evaluate approaches that combine the strengths of multiple models to achieve better overall performance.\n",
    "6. Conduct more in-depth analysis of misclassifications to identify specific areas for improvement.\n",
    "    \n",
    "<br> <br>\n",
    "    In conclusion, our project successfully identified influential factors that impact performance at the Olympics. The challenges we encountered have enriched our understanding of data science in general and enhanced our appreciation for the application of data science in the field of sports analytics.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58781a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
